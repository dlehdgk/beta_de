WARNING:root:outlier fraction 0.0002575328354365182 
./chains/PlanckLensingDESI.1.txt
./chains/PlanckLensingDESI.4.txt
./chains/PlanckLensingDESI.2.txt
./chains/PlanckLensingDESI.3.txt
Removed 0.3 as burn in
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 23 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 23 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 23 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 23 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 23 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 24 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 22 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/PlanckLensingDESI.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[warn] Epoll MOD(1) on fd 26 failed. Old events were 6; read change was 0 (none); write change was 2 (del); close change was 0 (none): Bad file descriptor
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] Found an old sample. Resuming.
[0 : prior] *WARNING* External prior 'SZ' loaded. Mind that it might not be normalized!
[0 : camb] `camb` module loaded successfully from /users/smp24dhl/cosmo/code/CAMB_beta/camb
[0 : planck_2018_highl_plik.ttteee] `clik` module loaded successfully from /users/smp24dhl/cosmo/code/planck/clik/lib/python3.9/site-packages/clik
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[1 : bao.desi_2024_bao_all] Initialized.
[2 : bao.desi_2024_bao_all] Initialized.
[3 : bao.desi_2024_bao_all] Initialized.
[0 : bao.desi_2024_bao_all] Initialized.
[0 : mcmc] Resuming from previous sample!
[0 : samplecollection] Loaded 1378 sample points from 'chains/PlanckLensingDESI.1.txt'
[3 : samplecollection] Loaded 1394 sample points from 'chains/PlanckLensingDESI.4.txt'
[1 : samplecollection] Loaded 1410 sample points from 'chains/PlanckLensingDESI.2.txt'
[2 : samplecollection] Loaded 1365 sample points from 'chains/PlanckLensingDESI.3.txt'
[0 : mcmc] Initial point: logA:3.031717, ns:0.9584374, theta_MC_100:1.040875, ombh2:0.02233636, omch2:0.1206665, w:-0.04216686, wa:-2.471444, beta_DE:0.1036251, tau:0.04751556, A_planck:0.9991804, calib_100T:0.9993007, calib_217T:0.9977517, A_cib_217:50.99776, xi_sz_cib:0.02367523, A_sz:2.847912, ksz_norm:4.180196, gal545_A_100:6.858394, gal545_A_143:10.00589, gal545_A_143_217:19.70591, gal545_A_217:93.16527, ps_A_100_100:294.8519, ps_A_143_143:50.31847, ps_A_143_217:26.31088, ps_A_217_217:104.0574, galf_TE_A_100:0.0889539, galf_TE_A_100_143:0.133348, galf_TE_A_100_217:0.4612602, galf_TE_A_143:0.2234116, galf_TE_A_143_217:0.6722984, galf_TE_A_217:2.21971
[3 : mcmc] Initial point: logA:3.028102, ns:0.9645385, theta_MC_100:1.040603, ombh2:0.02232883, omch2:0.120451, w:-0.2461635, wa:-2.068749, beta_DE:0.9222692, tau:0.04032077, A_planck:1.005221, calib_100T:0.9997787, calib_217T:0.9975124, A_cib_217:55.43768, xi_sz_cib:0.05514377, A_sz:5.36208, ksz_norm:0.7753706, gal545_A_100:9.333209, gal545_A_143:8.970783, gal545_A_143_217:12.50079, gal545_A_217:81.60558, ps_A_100_100:282.6611, ps_A_143_143:48.19684, ps_A_143_217:37.79163, ps_A_217_217:111.0502, galf_TE_A_100:0.1235769, galf_TE_A_100_143:0.09395247, galf_TE_A_100_217:0.4765327, galf_TE_A_143:0.1872233, galf_TE_A_143_217:0.6975248, galf_TE_A_217:2.199085
[2 : mcmc] Initial point: logA:3.050766, ns:0.9649762, theta_MC_100:1.040599, ombh2:0.02218785, omch2:0.1205773, w:-0.5824531, wa:-1.386786, beta_DE:1.694823, tau:0.05495656, A_planck:1.002009, calib_100T:0.999117, calib_217T:0.9989676, A_cib_217:47.63808, xi_sz_cib:0.3999334, A_sz:3.791511, ksz_norm:3.820549, gal545_A_100:10.75622, gal545_A_143:11.36888, gal545_A_143_217:17.94267, gal545_A_217:93.3982, ps_A_100_100:244.9203, ps_A_143_143:41.81741, ps_A_143_217:35.78107, ps_A_217_217:111.8049, galf_TE_A_100:0.1442873, galf_TE_A_100_143:0.07467853, galf_TE_A_100_217:0.4518731, galf_TE_A_143:0.2275629, galf_TE_A_143_217:0.5809676, galf_TE_A_217:2.09929
[1 : mcmc] Initial point: logA:3.034041, ns:0.9641298, theta_MC_100:1.041072, ombh2:0.02226321, omch2:0.1202456, w:-0.5852535, wa:-2.373811, beta_DE:3.39163, tau:0.04978148, A_planck:0.998901, calib_100T:0.9999662, calib_217T:0.9988731, A_cib_217:55.12864, xi_sz_cib:0.1843225, A_sz:2.606884, ksz_norm:4.383423, gal545_A_100:11.49857, gal545_A_143:8.203735, gal545_A_143_217:14.5111, gal545_A_217:87.80579, ps_A_100_100:279.2133, ps_A_143_143:44.95037, ps_A_143_217:31.62614, ps_A_217_217:99.8399, galf_TE_A_100:0.1190449, galf_TE_A_100_143:0.1467232, galf_TE_A_100_217:0.4609795, galf_TE_A_143:0.2598193, galf_TE_A_143_217:0.6545995, galf_TE_A_217:1.923429
[0 : mcmc] *WARNING* Parameter blocking manually/previously fixed: speeds will not be measured.
[0 : mcmc] Dragging with number of interpolating steps:
[0 : mcmc] *  1 : (['theta_MC_100', 'ombh2', 'omch2', 'w', 'wa', 'beta_DE', 'tau'], ['logA', 'ns'])
[0 : mcmc] * 12 : (['A_planck'], ['calib_100T', 'calib_217T', 'A_cib_217', 'xi_sz_cib', 'A_sz', 'ksz_norm', 'gal545_A_100', 'gal545_A_143', 'gal545_A_143_217', 'gal545_A_217', 'ps_A_100_100', 'ps_A_143_143', 'ps_A_143_217', 'ps_A_217_217', 'galf_TE_A_100', 'galf_TE_A_100_143', 'galf_TE_A_100_217', 'galf_TE_A_143', 'galf_TE_A_143_217', 'galf_TE_A_217'])
[0 : mcmc] Covariance matrix from previous sample.
[0 : mcmc] Sampling!
[0 : mcmc] Progress @ 2025-02-21 13:53:41 : 1 steps taken, and 1378 accepted.
[1 : mcmc] Progress @ 2025-02-21 13:53:41 : 1 steps taken, and 1410 accepted.
[3 : mcmc] Progress @ 2025-02-21 13:53:41 : 1 steps taken, and 1394 accepted.
[2 : mcmc] Progress @ 2025-02-21 13:54:09 : 1 steps taken, and 1365 accepted.
[1 : mcmc] Progress @ 2025-02-21 13:54:44 : 9 steps taken, and 1410 accepted.
[3 : mcmc] Progress @ 2025-02-21 13:54:54 : 8 steps taken, and 1394 accepted.
[0 : mcmc] Progress @ 2025-02-21 13:54:54 : 11 steps taken, and 1378 accepted.
[2 : mcmc] Progress @ 2025-02-21 13:55:11 : 8 steps taken, and 1365 accepted.
[node147:8756 :0:8756] Caught signal 11 (Segmentation fault: address not mapped to object at address 0xfffffffc042ff018)
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
==== backtrace (tid:   8756) ====
 0 0x0000000000021563 ucs_debug_print_backtrace()  /dev/shm/UCX/1.8.0/GCCcore-9.3.0/ucx-1.8.0/src/ucs/debug/debug.c:653
 1 0x0000000000039f63 __massivenu_MOD_thermalnubackground_rho()  ???:0
 2 0x000000000003da49 __results_MOD_grho_no_de()  ???:0
 3 0x00000000000a5f01 dtauda_()  ???:0
 4 0x000000000004cfdc __results_MOD_thermo_init._omp_fn.0()  results.f90:0
 5 0x0000000000014295 GOMP_parallel_sections()  ???:0
 6 0x000000000004f7c2 __results_MOD_thermo_init()  ???:0
 7 0x00000000000f3923 __cambmain_MOD_initvars()  ???:0
 8 0x00000000000f9bff __cambmain_MOD_cmbmain()  ???:0
 9 0x0000000000105846 __camb_MOD_camb_getresults()  ???:0
10 0x00000000001104df __handles_MOD_cambdata_gettransfers()  ???:0
11 0x0000000000006a4a ffi_call_unix64()  :0
12 0x0000000000005fea ffi_call_int()  ffi64.c:0
13 0x00000000000091e0 _ctypes_callproc.cold()  :0
14 0x00000000000126ee PyCFuncPtr_call()  :0
15 0x00000000004f061c _PyObject_MakeTpCall()  ???:0
16 0x00000000004ec4f7 _PyEval_EvalFrameDefault()  ???:0
17 0x00000000004e694a _PyEval_EvalCode()  :0
18 0x000000000050500d method_vectorcall()  :0
19 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
20 0x00000000004e694a _PyEval_EvalCode()  :0
21 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
22 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
23 0x00000000004e694a _PyEval_EvalCode()  :0
24 0x000000000050500d method_vectorcall()  :0
25 0x0000000000505744 PyObject_Call()  ???:0
26 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
27 0x00000000004e694a _PyEval_EvalCode()  :0
28 0x000000000050500d method_vectorcall()  :0
29 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
30 0x00000000004e694a _PyEval_EvalCode()  :0
31 0x000000000050500d method_vectorcall()  :0
32 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
33 0x00000000004e694a _PyEval_EvalCode()  :0
34 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
35 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
36 0x00000000004f8073 function_code_fastcall()  :0
37 0x0000000000504f01 method_vectorcall()  :0
38 0x00000000004ec5b4 _PyEval_EvalFrameDefault()  ???:0
39 0x00000000004f8073 function_code_fastcall()  :0
40 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
41 0x00000000004e694a _PyEval_EvalCode()  :0
42 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
43 0x0000000000505744 PyObject_Call()  ???:0
44 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
45 0x00000000004f8073 function_code_fastcall()  :0
46 0x00000000004e7c22 _PyEval_EvalFrameDefault()  ???:0
47 0x00000000004e694a _PyEval_EvalCode()  :0
48 0x00000000004e65d7 _PyEval_EvalCodeWithName()  ???:0
49 0x00000000004e6589 PyEval_EvalCodeEx()  ???:0
50 0x0000000000593f2b PyEval_EvalCode()  ???:0
51 0x00000000005c1697 run_eval_code_obj()  :0
52 0x00000000005bd6b0 run_mod()  :0
53 0x00000000004565eb pyrun_file.cold()  :0
54 0x00000000005b7392 PyRun_SimpleFileExFlags()  ???:0
55 0x00000000005b490e Py_RunMain()  ???:0
56 0x0000000000587fd9 Py_BytesMain()  ???:0
57 0x0000000000022555 __libc_start_main()  ???:0
=================================
[node147:08756] *** Process received signal ***
[node147:08756] Signal: Segmentation fault (11)
[node147:08756] Signal code:  (-6)
[node147:08756] Failing at address: 0x8cfb800002234
[node147:08756] [ 0] /lib64/libpthread.so.0(+0xf630)[0x7f16b1dfd630]
[node147:08756] [ 1] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__massivenu_MOD_thermalnubackground_rho+0x63)[0x7f16906eff63]
[node147:08756] [ 2] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_grho_no_de+0xc9)[0x7f16906f3a49]
[node147:08756] [ 3] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(dtauda_+0x61)[0x7f169075bf01]
[node147:08756] [ 4] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(+0x4cfdc)[0x7f1690702fdc]
[node147:08756] [ 5] /users/smp24dhl/.conda/envs/cosmos/bin/../lib/libgomp.so.1(GOMP_parallel_sections+0x80)[0x7f169068b295]
[node147:08756] [ 6] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_thermo_init+0x12d2)[0x7f16907057c2]
[node147:08756] [ 7] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_initvars+0x193)[0x7f16907a9923]
[node147:08756] [ 8] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_cmbmain+0xbf)[0x7f16907afbff]
[node147:08756] [ 9] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__camb_MOD_camb_getresults+0x2986)[0x7f16907bb846]
[node147:08756] [10] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__handles_MOD_cambdata_gettransfers+0x1f)[0x7f16907c64df]
[node147:08756] [11] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x6a4a)[0x7f16b20b8a4a]
[node147:08756] [12] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x5fea)[0x7f16b20b7fea]
[node147:08756] [13] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x91e0)[0x7f16a7db71e0]
[node147:08756] [14] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x126ee)[0x7f16a7dc06ee]
[node147:08756] [15] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyObject_MakeTpCall+0x2ec)[0x4f061c]
[node147:08756] [16] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x4ca7)[0x4ec4f7]
[node147:08756] [17] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:08756] [18] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:08756] [19] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:08756] [20] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:08756] [21] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyFunction_Vectorcall+0xd5)[0x4f7d95]
[node147:08756] [22] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:08756] [23] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:08756] [24] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:08756] [25] /users/smp24dhl/.conda/envs/cosmos/bin/python(PyObject_Call+0xb4)[0x505744]
[node147:08756] [26] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x3418)[0x4eac68]
[node147:08756] [27] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:08756] [28] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:08756] [29] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:08756] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 8756 on node node147 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
WARNING:root:outlier fraction 0.0002575328354365182 
./chains/PlanckLensingDESI.1.txt
./chains/PlanckLensingDESI.4.txt
./chains/PlanckLensingDESI.2.txt
./chains/PlanckLensingDESI.3.txt
Removed 0.3 as burn in
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.00047201352143533753,N_eff=1062.6105730892589). Using fallback (h=0.09768400399824362)
R-1 = 2.1584563844280336
H_0 = 66.3^{+3.0}_{-3.4}
w_{0,\mathrm{DE}} = -0.60^{+0.33}_{-0.27}
w_{a,\mathrm{DE}} < -1.41
\beta_{\mathrm{DE}} = 2.61^{+0.87}_{-2.3}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] Found an old sample. Resuming.
[0 : prior] *WARNING* External prior 'SZ' loaded. Mind that it might not be normalized!
[0 : camb] `camb` module loaded successfully from /users/smp24dhl/cosmo/code/CAMB_beta/camb
[0 : planck_2018_highl_plik.ttteee] `clik` module loaded successfully from /users/smp24dhl/cosmo/code/planck/clik/lib/python3.9/site-packages/clik
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[2 : bao.desi_2024_bao_all] Initialized.
[0 : bao.desi_2024_bao_all] Initialized.
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[1 : bao.desi_2024_bao_all] Initialized.
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[3 : bao.desi_2024_bao_all] Initialized.
[0 : mcmc] Resuming from previous sample!
[0 : samplecollection] Loaded 1378 sample points from 'chains/PlanckLensingDESI.1.txt'
[1 : samplecollection] Loaded 1410 sample points from 'chains/PlanckLensingDESI.2.txt'
[2 : samplecollection] Loaded 1365 sample points from 'chains/PlanckLensingDESI.3.txt'
[3 : samplecollection] Loaded 1394 sample points from 'chains/PlanckLensingDESI.4.txt'
[0 : mcmc] Initial point: logA:3.031717, ns:0.9584374, theta_MC_100:1.040875, ombh2:0.02233636, omch2:0.1206665, w:-0.04216686, wa:-2.471444, beta_DE:0.1036251, tau:0.04751556, A_planck:0.9991804, calib_100T:0.9993007, calib_217T:0.9977517, A_cib_217:50.99776, xi_sz_cib:0.02367523, A_sz:2.847912, ksz_norm:4.180196, gal545_A_100:6.858394, gal545_A_143:10.00589, gal545_A_143_217:19.70591, gal545_A_217:93.16527, ps_A_100_100:294.8519, ps_A_143_143:50.31847, ps_A_143_217:26.31088, ps_A_217_217:104.0574, galf_TE_A_100:0.0889539, galf_TE_A_100_143:0.133348, galf_TE_A_100_217:0.4612602, galf_TE_A_143:0.2234116, galf_TE_A_143_217:0.6722984, galf_TE_A_217:2.21971
[2 : mcmc] Initial point: logA:3.050766, ns:0.9649762, theta_MC_100:1.040599, ombh2:0.02218785, omch2:0.1205773, w:-0.5824531, wa:-1.386786, beta_DE:1.694823, tau:0.05495656, A_planck:1.002009, calib_100T:0.999117, calib_217T:0.9989676, A_cib_217:47.63808, xi_sz_cib:0.3999334, A_sz:3.791511, ksz_norm:3.820549, gal545_A_100:10.75622, gal545_A_143:11.36888, gal545_A_143_217:17.94267, gal545_A_217:93.3982, ps_A_100_100:244.9203, ps_A_143_143:41.81741, ps_A_143_217:35.78107, ps_A_217_217:111.8049, galf_TE_A_100:0.1442873, galf_TE_A_100_143:0.07467853, galf_TE_A_100_217:0.4518731, galf_TE_A_143:0.2275629, galf_TE_A_143_217:0.5809676, galf_TE_A_217:2.09929
[1 : mcmc] Initial point: logA:3.034041, ns:0.9641298, theta_MC_100:1.041072, ombh2:0.02226321, omch2:0.1202456, w:-0.5852535, wa:-2.373811, beta_DE:3.39163, tau:0.04978148, A_planck:0.998901, calib_100T:0.9999662, calib_217T:0.9988731, A_cib_217:55.12864, xi_sz_cib:0.1843225, A_sz:2.606884, ksz_norm:4.383423, gal545_A_100:11.49857, gal545_A_143:8.203735, gal545_A_143_217:14.5111, gal545_A_217:87.80579, ps_A_100_100:279.2133, ps_A_143_143:44.95037, ps_A_143_217:31.62614, ps_A_217_217:99.8399, galf_TE_A_100:0.1190449, galf_TE_A_100_143:0.1467232, galf_TE_A_100_217:0.4609795, galf_TE_A_143:0.2598193, galf_TE_A_143_217:0.6545995, galf_TE_A_217:1.923429
[3 : mcmc] Initial point: logA:3.028102, ns:0.9645385, theta_MC_100:1.040603, ombh2:0.02232883, omch2:0.120451, w:-0.2461635, wa:-2.068749, beta_DE:0.9222692, tau:0.04032077, A_planck:1.005221, calib_100T:0.9997787, calib_217T:0.9975124, A_cib_217:55.43768, xi_sz_cib:0.05514377, A_sz:5.36208, ksz_norm:0.7753706, gal545_A_100:9.333209, gal545_A_143:8.970783, gal545_A_143_217:12.50079, gal545_A_217:81.60558, ps_A_100_100:282.6611, ps_A_143_143:48.19684, ps_A_143_217:37.79163, ps_A_217_217:111.0502, galf_TE_A_100:0.1235769, galf_TE_A_100_143:0.09395247, galf_TE_A_100_217:0.4765327, galf_TE_A_143:0.1872233, galf_TE_A_143_217:0.6975248, galf_TE_A_217:2.199085
[0 : mcmc] *WARNING* Parameter blocking manually/previously fixed: speeds will not be measured.
[0 : mcmc] Dragging with number of interpolating steps:
[0 : mcmc] *  1 : (['theta_MC_100', 'ombh2', 'omch2', 'w', 'wa', 'beta_DE', 'tau'], ['logA', 'ns'])
[0 : mcmc] * 12 : (['A_planck'], ['calib_100T', 'calib_217T', 'A_cib_217', 'xi_sz_cib', 'A_sz', 'ksz_norm', 'gal545_A_100', 'gal545_A_143', 'gal545_A_143_217', 'gal545_A_217', 'ps_A_100_100', 'ps_A_143_143', 'ps_A_143_217', 'ps_A_217_217', 'galf_TE_A_100', 'galf_TE_A_100_143', 'galf_TE_A_100_217', 'galf_TE_A_143', 'galf_TE_A_143_217', 'galf_TE_A_217'])
[0 : mcmc] Covariance matrix from previous sample.
[0 : mcmc] Sampling!
[0 : mcmc] Progress @ 2025-02-21 13:56:00 : 1 steps taken, and 1378 accepted.
[2 : mcmc] Progress @ 2025-02-21 13:56:19 : 1 steps taken, and 1365 accepted.
[1 : mcmc] Progress @ 2025-02-21 13:56:29 : 1 steps taken, and 1410 accepted.
[3 : mcmc] Progress @ 2025-02-21 13:56:29 : 1 steps taken, and 1394 accepted.
[0 : mcmc] Progress @ 2025-02-21 13:57:00 : 8 steps taken, and 1379 accepted.
[1 : mcmc] Progress @ 2025-02-21 13:57:30 : 9 steps taken, and 1410 accepted.
[2 : mcmc] Progress @ 2025-02-21 13:57:31 : 7 steps taken, and 1366 accepted.
[3 : mcmc] Progress @ 2025-02-21 13:57:31 : 7 steps taken, and 1396 accepted.
[0 : mcmc] Progress @ 2025-02-21 13:58:02 : 15 steps taken, and 1380 accepted.
[1 : mcmc] Progress @ 2025-02-21 13:58:32 : 17 steps taken, and 1410 accepted.
[3 : mcmc] Progress @ 2025-02-21 13:58:39 : 13 steps taken, and 1397 accepted.
[2 : mcmc] Progress @ 2025-02-21 13:58:42 : 15 steps taken, and 1368 accepted.
[0 : mcmc] Progress @ 2025-02-21 13:59:09 : 23 steps taken, and 1382 accepted.
[1 : mcmc] Progress @ 2025-02-21 13:59:36 : 33 steps taken, and 1410 accepted.
[2 : mcmc] Progress @ 2025-02-21 13:59:42 : 23 steps taken, and 1371 accepted.
[3 : mcmc] Progress @ 2025-02-21 13:59:44 : 21 steps taken, and 1400 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:00:22 : 35 steps taken, and 1385 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:00:36 : 42 steps taken, and 1411 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:00:42 : 31 steps taken, and 1372 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:00:55 : 26 steps taken, and 1401 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:01:26 : 42 steps taken, and 1385 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:01:41 : 52 steps taken, and 1412 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:01:50 : 40 steps taken, and 1372 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:02:03 : 34 steps taken, and 1402 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:02:27 : 49 steps taken, and 1387 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:02:45 : 62 steps taken, and 1413 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:03:02 : 46 steps taken, and 1374 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:03:08 : 40 steps taken, and 1405 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:03:36 : 58 steps taken, and 1389 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:03:45 : 67 steps taken, and 1414 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:04:06 : 53 steps taken, and 1375 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:04:19 : 46 steps taken, and 1406 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:04:42 : 67 steps taken, and 1390 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:04:57 : 73 steps taken, and 1415 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:05:06 : 63 steps taken, and 1376 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:05:30 : 52 steps taken, and 1407 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:05:49 : 75 steps taken, and 1390 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:06:08 : 70 steps taken, and 1378 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:06:08 : 80 steps taken, and 1417 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:06:31 : 60 steps taken, and 1407 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:06:50 : 81 steps taken, and 1390 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:07:09 : 89 steps taken, and 1418 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:07:09 : 78 steps taken, and 1379 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:07:32 : 67 steps taken, and 1409 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:08:01 : 89 steps taken, and 1392 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:08:10 : 99 steps taken, and 1419 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:08:17 : 84 steps taken, and 1379 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:08:45 : 74 steps taken, and 1411 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:09:12 : 95 steps taken, and 1393 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:09:14 : 106 steps taken, and 1422 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:09:19 : 89 steps taken, and 1380 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:09:46 : 80 steps taken, and 1413 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:10:13 : 101 steps taken, and 1394 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:10:25 : 111 steps taken, and 1422 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:10:28 : 97 steps taken, and 1381 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:10:51 : 89 steps taken, and 1415 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:11:13 : 106 steps taken, and 1395 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:11:29 : 103 steps taken, and 1383 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:11:30 : 119 steps taken, and 1422 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:12:03 : 94 steps taken, and 1416 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:12:14 : 112 steps taken, and 1397 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:12:30 : 109 steps taken, and 1385 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:12:31 : 127 steps taken, and 1424 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:13:11 : 102 steps taken, and 1418 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:13:15 : 118 steps taken, and 1400 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:13:30 : 114 steps taken, and 1385 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:13:33 : 134 steps taken, and 1426 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:14:15 : 125 steps taken, and 1402 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:14:23 : 108 steps taken, and 1419 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:14:32 : 121 steps taken, and 1386 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:14:41 : 141 steps taken, and 1427 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:15:15 : 132 steps taken, and 1405 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:15:24 : 118 steps taken, and 1422 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:15:33 : 128 steps taken, and 1387 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:15:44 : 153 steps taken, and 1429 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:16:16 : 139 steps taken, and 1406 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:16:31 : 123 steps taken, and 1423 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:16:44 : 135 steps taken, and 1389 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:16:45 : 161 steps taken, and 1433 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:17:18 : 148 steps taken, and 1407 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:17:32 : 129 steps taken, and 1424 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:17:45 : 141 steps taken, and 1390 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:17:46 : 170 steps taken, and 1435 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:18:27 : 153 steps taken, and 1408 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:18:33 : 137 steps taken, and 1425 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:18:57 : 176 steps taken, and 1437 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:18:57 : 148 steps taken, and 1390 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:19:29 : 160 steps taken, and 1411 accepted.
[1 : mcmc] Learn + convergence test @ 1440 samples accepted.
[1 : mcmc] Ready to check convergence and learn a new proposal covmat (waiting for the rest...)
[3 : mcmc] Progress @ 2025-02-21 14:19:45 : 144 steps taken, and 1426 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:19:58 : 183 steps taken, and 1442 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:19:58 : 154 steps taken, and 1390 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:20:37 : 165 steps taken, and 1413 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:20:47 : 150 steps taken, and 1429 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:21:07 : 159 steps taken, and 1390 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:21:09 : 190 steps taken, and 1442 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:21:50 : 172 steps taken, and 1415 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:21:58 : 158 steps taken, and 1431 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:22:08 : 166 steps taken, and 1391 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:22:11 : 199 steps taken, and 1445 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:23:01 : 185 steps taken, and 1415 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:23:10 : 167 steps taken, and 1432 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:23:18 : 205 steps taken, and 1446 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:23:19 : 172 steps taken, and 1393 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:24:12 : 200 steps taken, and 1416 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:24:15 : 176 steps taken, and 1433 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:24:19 : 212 steps taken, and 1448 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:24:30 : 178 steps taken, and 1394 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:25:16 : 185 steps taken, and 1433 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:25:19 : 220 steps taken, and 1450 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:25:21 : 208 steps taken, and 1418 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:25:42 : 185 steps taken, and 1395 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:26:21 : 196 steps taken, and 1437 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:26:22 : 217 steps taken, and 1420 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:26:26 : 228 steps taken, and 1451 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:26:52 : 195 steps taken, and 1396 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:27:23 : 224 steps taken, and 1423 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:27:29 : 201 steps taken, and 1439 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:27:37 : 235 steps taken, and 1453 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:28:04 : 201 steps taken, and 1398 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:28:31 : 207 steps taken, and 1439 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:28:34 : 232 steps taken, and 1426 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:28:49 : 241 steps taken, and 1456 accepted.
[3 : mcmc] Learn + convergence test @ 1440 samples accepted.
[3 : mcmc] Ready to check convergence and learn a new proposal covmat (waiting for the rest...)
[2 : mcmc] Progress @ 2025-02-21 14:29:05 : 207 steps taken, and 1399 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:29:32 : 213 steps taken, and 1442 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:29:45 : 239 steps taken, and 1428 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:30:00 : 247 steps taken, and 1457 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:30:06 : 214 steps taken, and 1403 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:30:41 : 220 steps taken, and 1444 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:30:50 : 248 steps taken, and 1431 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:31:01 : 254 steps taken, and 1457 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:31:14 : 220 steps taken, and 1405 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:31:42 : 226 steps taken, and 1446 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:32:00 : 255 steps taken, and 1432 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:32:13 : 260 steps taken, and 1460 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:32:18 : 228 steps taken, and 1407 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:32:44 : 232 steps taken, and 1446 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:33:01 : 263 steps taken, and 1434 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:33:24 : 267 steps taken, and 1462 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:33:28 : 235 steps taken, and 1408 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:33:56 : 239 steps taken, and 1451 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:34:09 : 269 steps taken, and 1435 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:34:27 : 276 steps taken, and 1462 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:34:36 : 245 steps taken, and 1410 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:35:07 : 246 steps taken, and 1454 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:35:14 : 279 steps taken, and 1437 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:35:28 : 282 steps taken, and 1464 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:35:38 : 251 steps taken, and 1413 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:36:08 : 254 steps taken, and 1456 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:36:26 : 287 steps taken, and 1439 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:36:28 : 290 steps taken, and 1465 accepted.
[0 : mcmc] Learn + convergence test @ 1440 samples accepted.
[0 : mcmc] Ready to check convergence and learn a new proposal covmat (waiting for the rest...)
[2 : mcmc] Progress @ 2025-02-21 14:36:49 : 257 steps taken, and 1415 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:37:10 : 260 steps taken, and 1457 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:37:29 : 298 steps taken, and 1468 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:37:30 : 298 steps taken, and 1440 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:38:00 : 264 steps taken, and 1415 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:38:18 : 265 steps taken, and 1459 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:38:32 : 310 steps taken, and 1442 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:38:40 : 304 steps taken, and 1468 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:39:00 : 271 steps taken, and 1415 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:39:23 : 274 steps taken, and 1461 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:39:33 : 318 steps taken, and 1444 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:39:51 : 310 steps taken, and 1469 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:40:01 : 279 steps taken, and 1417 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:40:34 : 282 steps taken, and 1465 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:40:40 : 328 steps taken, and 1445 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:41:02 : 316 steps taken, and 1471 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:41:02 : 286 steps taken, and 1418 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:41:38 : 292 steps taken, and 1467 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:41:52 : 335 steps taken, and 1449 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:42:03 : 322 steps taken, and 1472 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:42:10 : 292 steps taken, and 1418 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:42:49 : 299 steps taken, and 1468 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:42:58 : 343 steps taken, and 1449 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:43:11 : 327 steps taken, and 1473 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:43:14 : 300 steps taken, and 1418 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:43:50 : 310 steps taken, and 1471 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:44:10 : 349 steps taken, and 1450 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:44:13 : 334 steps taken, and 1475 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:44:25 : 306 steps taken, and 1419 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:44:58 : 316 steps taken, and 1473 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:45:10 : 355 steps taken, and 1450 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:45:13 : 341 steps taken, and 1477 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:45:35 : 312 steps taken, and 1422 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:45:59 : 323 steps taken, and 1475 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:46:21 : 362 steps taken, and 1451 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:46:22 : 346 steps taken, and 1479 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:46:37 : 321 steps taken, and 1424 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:47:02 : 331 steps taken, and 1476 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:47:22 : 367 steps taken, and 1454 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:47:26 : 352 steps taken, and 1480 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:47:41 : 330 steps taken, and 1426 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:48:05 : 339 steps taken, and 1478 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:48:22 : 372 steps taken, and 1454 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:48:30 : 359 steps taken, and 1481 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:48:51 : 336 steps taken, and 1427 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:49:09 : 347 steps taken, and 1479 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:49:22 : 377 steps taken, and 1456 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:49:30 : 364 steps taken, and 1482 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:49:59 : 344 steps taken, and 1430 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:50:23 : 353 steps taken, and 1482 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:50:30 : 387 steps taken, and 1457 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:50:30 : 370 steps taken, and 1484 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:50:59 : 350 steps taken, and 1431 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:51:24 : 358 steps taken, and 1483 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:51:30 : 393 steps taken, and 1459 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:51:43 : 376 steps taken, and 1485 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:52:02 : 360 steps taken, and 1433 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:52:24 : 364 steps taken, and 1484 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:52:33 : 404 steps taken, and 1459 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:52:50 : 384 steps taken, and 1488 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:53:06 : 366 steps taken, and 1433 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:53:24 : 369 steps taken, and 1485 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:53:35 : 410 steps taken, and 1461 accepted.
 Warning: Integrate_Romberg failed to converge; 
 integral, error, tol:   13253.177325639037       -3.2156547757900000E-007   1.0000000000000001E-007
[node147:9542 :0:9542] Caught signal 11 (Segmentation fault: address not mapped to object at address 0xfffffffc0538b0e8)
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
==== backtrace (tid:   9542) ====
 0 0x0000000000021563 ucs_debug_print_backtrace()  /dev/shm/UCX/1.8.0/GCCcore-9.3.0/ucx-1.8.0/src/ucs/debug/debug.c:653
 1 0x0000000000039f63 __massivenu_MOD_thermalnubackground_rho()  ???:0
 2 0x000000000003da49 __results_MOD_grho_no_de()  ???:0
 3 0x00000000000a5f01 dtauda_()  ???:0
 4 0x000000000004cfdc __results_MOD_thermo_init._omp_fn.0()  results.f90:0
 5 0x0000000000014295 GOMP_parallel_sections()  ???:0
 6 0x000000000004f7c2 __results_MOD_thermo_init()  ???:0
 7 0x00000000000f3923 __cambmain_MOD_initvars()  ???:0
 8 0x00000000000f9bff __cambmain_MOD_cmbmain()  ???:0
 9 0x0000000000105846 __camb_MOD_camb_getresults()  ???:0
10 0x00000000001104df __handles_MOD_cambdata_gettransfers()  ???:0
11 0x0000000000006a4a ffi_call_unix64()  :0
12 0x0000000000005fea ffi_call_int()  ffi64.c:0
13 0x00000000000091e0 _ctypes_callproc.cold()  :0
14 0x00000000000126ee PyCFuncPtr_call()  :0
15 0x00000000004f061c _PyObject_MakeTpCall()  ???:0
16 0x00000000004ec4f7 _PyEval_EvalFrameDefault()  ???:0
17 0x00000000004e694a _PyEval_EvalCode()  :0
18 0x000000000050500d method_vectorcall()  :0
19 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
20 0x00000000004e694a _PyEval_EvalCode()  :0
21 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
22 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
23 0x00000000004e694a _PyEval_EvalCode()  :0
24 0x000000000050500d method_vectorcall()  :0
25 0x0000000000505744 PyObject_Call()  ???:0
26 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
27 0x00000000004e694a _PyEval_EvalCode()  :0
28 0x000000000050500d method_vectorcall()  :0
29 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
30 0x00000000004e694a _PyEval_EvalCode()  :0
31 0x000000000050500d method_vectorcall()  :0
32 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
33 0x00000000004e694a _PyEval_EvalCode()  :0
34 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
35 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
36 0x00000000004f8073 function_code_fastcall()  :0
37 0x0000000000504f01 method_vectorcall()  :0
38 0x00000000004ec5b4 _PyEval_EvalFrameDefault()  ???:0
39 0x00000000004f8073 function_code_fastcall()  :0
40 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
41 0x00000000004e694a _PyEval_EvalCode()  :0
42 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
43 0x0000000000505744 PyObject_Call()  ???:0
44 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
45 0x00000000004f8073 function_code_fastcall()  :0
46 0x00000000004e7c22 _PyEval_EvalFrameDefault()  ???:0
47 0x00000000004e694a _PyEval_EvalCode()  :0
48 0x00000000004e65d7 _PyEval_EvalCodeWithName()  ???:0
49 0x00000000004e6589 PyEval_EvalCodeEx()  ???:0
50 0x0000000000593f2b PyEval_EvalCode()  ???:0
51 0x00000000005c1697 run_eval_code_obj()  :0
52 0x00000000005bd6b0 run_mod()  :0
53 0x00000000004565eb pyrun_file.cold()  :0
54 0x00000000005b7392 PyRun_SimpleFileExFlags()  ???:0
55 0x00000000005b490e Py_RunMain()  ???:0
56 0x0000000000587fd9 Py_BytesMain()  ???:0
57 0x0000000000022555 __libc_start_main()  ???:0
=================================
[node147:09542] *** Process received signal ***
[node147:09542] Signal: Segmentation fault (11)
[node147:09542] Signal code:  (-6)
[node147:09542] Failing at address: 0x8cfb800002546
[node147:09542] [ 0] /lib64/libpthread.so.0(+0xf630)[0x7f07f46d6630]
[node147:09542] [ 1] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__massivenu_MOD_thermalnubackground_rho+0x63)[0x7f07d2fd8f63]
[node147:09542] [ 2] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_grho_no_de+0xc9)[0x7f07d2fdca49]
[node147:09542] [ 3] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(dtauda_+0x61)[0x7f07d3044f01]
[node147:09542] [ 4] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(+0x4cfdc)[0x7f07d2febfdc]
[node147:09542] [ 5] /users/smp24dhl/.conda/envs/cosmos/bin/../lib/libgomp.so.1(GOMP_parallel_sections+0x80)[0x7f07d2f74295]
[node147:09542] [ 6] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_thermo_init+0x12d2)[0x7f07d2fee7c2]
[node147:09542] [ 7] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_initvars+0x193)[0x7f07d3092923]
[node147:09542] [ 8] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_cmbmain+0xbf)[0x7f07d3098bff]
[node147:09542] [ 9] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__camb_MOD_camb_getresults+0x2986)[0x7f07d30a4846]
[node147:09542] [10] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__handles_MOD_cambdata_gettransfers+0x1f)[0x7f07d30af4df]
[node147:09542] [11] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x6a4a)[0x7f07f4991a4a]
[node147:09542] [12] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x5fea)[0x7f07f4990fea]
[node147:09542] [13] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x91e0)[0x7f07ea6901e0]
[node147:09542] [14] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x126ee)[0x7f07ea6996ee]
[node147:09542] [15] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyObject_MakeTpCall+0x2ec)[0x4f061c]
[node147:09542] [16] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x4ca7)[0x4ec4f7]
[node147:09542] [17] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:09542] [18] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:09542] [19] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:09542] [20] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:09542] [21] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyFunction_Vectorcall+0xd5)[0x4f7d95]
[node147:09542] [22] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:09542] [23] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:09542] [24] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:09542] [25] /users/smp24dhl/.conda/envs/cosmos/bin/python(PyObject_Call+0xb4)[0x505744]
[node147:09542] [26] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x3418)[0x4eac68]
[node147:09542] [27] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:09542] [28] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:09542] [29] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:09542] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 9542 on node node147 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
./chains/PlanckLensingDESI.1.txt
./chains/PlanckLensingDESI.4.txt
./chains/PlanckLensingDESI.2.txt
./chains/PlanckLensingDESI.3.txt
Removed 0.3 as burn in
WARNING:root:auto bandwidth for xi_sz_cib very small or failed (h=0.0005015561453117379,N_eff=1142.818100622138). Using fallback (h=0.09412681238343155)
R-1 = 1.876958714954279
H_0 = 66.3^{+2.9}_{-3.3}
w_{0,\mathrm{DE}} = -0.59^{+0.31}_{-0.26}
w_{a,\mathrm{DE}} < -1.44
\beta_{\mathrm{DE}} = 2.60^{+0.87}_{-2.2}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] Found an old sample. Resuming.
[0 : prior] *WARNING* External prior 'SZ' loaded. Mind that it might not be normalized!
[0 : camb] `camb` module loaded successfully from /users/smp24dhl/cosmo/code/CAMB_beta/camb
[0 : planck_2018_highl_plik.ttteee] `clik` module loaded successfully from /users/smp24dhl/cosmo/code/planck/clik/lib/python3.9/site-packages/clik
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[1 : bao.desi_2024_bao_all] Initialized.
[0 : bao.desi_2024_bao_all] Initialized.
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[0 : mcmc] Resuming from previous sample!
[2 : bao.desi_2024_bao_all] Initialized.
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[3 : bao.desi_2024_bao_all] Initialized.
[0 : samplecollection] Loaded 1461 sample points from 'chains/PlanckLensingDESI.1.txt'
[2 : samplecollection] Loaded 1433 sample points from 'chains/PlanckLensingDESI.3.txt'
[3 : samplecollection] Loaded 1485 sample points from 'chains/PlanckLensingDESI.4.txt'
[1 : samplecollection] Loaded 1488 sample points from 'chains/PlanckLensingDESI.2.txt'
[0 : mcmc] Initial point: logA:3.047267, ns:0.9630649, theta_MC_100:1.040799, ombh2:0.02249668, omch2:0.1206933, w:-1.266052, wa:0.2466196, beta_DE:0.2119656, tau:0.05374682, A_planck:1.000554, calib_100T:0.999905, calib_217T:0.998504, A_cib_217:38.08497, xi_sz_cib:0.9803141, A_sz:6.451356, ksz_norm:0.5823426, gal545_A_100:7.679063, gal545_A_143:9.092155, gal545_A_143_217:19.14576, gal545_A_217:99.79495, ps_A_100_100:250.9802, ps_A_143_143:53.93953, ps_A_143_217:61.58546, ps_A_217_217:133.2526, galf_TE_A_100:0.05908609, galf_TE_A_100_143:0.1067883, galf_TE_A_100_217:0.4588737, galf_TE_A_143:0.2019224, galf_TE_A_143_217:0.7669022, galf_TE_A_217:2.285864
[2 : mcmc] Initial point: logA:3.07628, ns:0.9722162, theta_MC_100:1.040704, ombh2:0.02255832, omch2:0.1176796, w:-0.5773876, wa:-2.568455, beta_DE:2.93008, tau:0.07344366, A_planck:1.000495, calib_100T:0.9996001, calib_217T:0.9982363, A_cib_217:41.69803, xi_sz_cib:0.5212489, A_sz:4.905729, ksz_norm:2.52614, gal545_A_100:11.05073, gal545_A_143:10.53399, gal545_A_143_217:19.54519, gal545_A_217:97.15001, ps_A_100_100:261.4454, ps_A_143_143:45.52146, ps_A_143_217:38.95972, ps_A_217_217:124.209, galf_TE_A_100:0.1744742, galf_TE_A_100_143:0.1302976, galf_TE_A_100_217:0.535469, galf_TE_A_143:0.218444, galf_TE_A_143_217:0.5336129, galf_TE_A_217:1.812711
[3 : mcmc] Initial point: logA:3.05911, ns:0.9627111, theta_MC_100:1.04073, ombh2:0.02243199, omch2:0.119512, w:-0.6751288, wa:-1.733642, beta_DE:3.270094, tau:0.05865978, A_planck:1.002454, calib_100T:0.9985157, calib_217T:0.9980999, A_cib_217:36.33165, xi_sz_cib:0.7991997, A_sz:3.064656, ksz_norm:2.776228, gal545_A_100:5.251164, gal545_A_143:10.7159, gal545_A_143_217:16.85687, gal545_A_217:98.94947, ps_A_100_100:277.7717, ps_A_143_143:55.52966, ps_A_143_217:50.42531, ps_A_217_217:134.661, galf_TE_A_100:0.1307663, galf_TE_A_100_143:0.1430835, galf_TE_A_100_217:0.464716, galf_TE_A_143:0.2102255, galf_TE_A_143_217:0.5307813, galf_TE_A_217:2.380746
[1 : mcmc] Initial point: logA:3.048399, ns:0.9630723, theta_MC_100:1.040556, ombh2:0.02228412, omch2:0.1198475, w:-0.9467498, wa:-1.363542, beta_DE:4.245749, tau:0.0585111, A_planck:0.9985874, calib_100T:1.000006, calib_217T:0.998675, A_cib_217:40.99744, xi_sz_cib:0.1861508, A_sz:5.939459, ksz_norm:0.3274506, gal545_A_100:11.51117, gal545_A_143:11.14254, gal545_A_143_217:19.76009, gal545_A_217:101.0699, ps_A_100_100:279.8361, ps_A_143_143:49.3641, ps_A_143_217:44.3648, ps_A_217_217:128.2594, galf_TE_A_100:0.1050033, galf_TE_A_100_143:0.1784152, galf_TE_A_100_217:0.5259761, galf_TE_A_143:0.3103235, galf_TE_A_143_217:0.7208887, galf_TE_A_217:2.230654
[0 : mcmc] *WARNING* Parameter blocking manually/previously fixed: speeds will not be measured.
[0 : mcmc] Dragging with number of interpolating steps:
[0 : mcmc] *  1 : (['theta_MC_100', 'ombh2', 'omch2', 'w', 'wa', 'beta_DE', 'tau'], ['logA', 'ns'])
[0 : mcmc] * 12 : (['A_planck'], ['calib_100T', 'calib_217T', 'A_cib_217', 'xi_sz_cib', 'A_sz', 'ksz_norm', 'gal545_A_100', 'gal545_A_143', 'gal545_A_143_217', 'gal545_A_217', 'ps_A_100_100', 'ps_A_143_143', 'ps_A_143_217', 'ps_A_217_217', 'galf_TE_A_100', 'galf_TE_A_100_143', 'galf_TE_A_100_217', 'galf_TE_A_143', 'galf_TE_A_143_217', 'galf_TE_A_217'])
[0 : mcmc] Covariance matrix from previous sample.
[0 : mcmc] Sampling!
[0 : mcmc] Progress @ 2025-02-21 14:54:00 : 1 steps taken, and 1461 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:54:19 : 1 steps taken, and 1488 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:54:30 : 1 steps taken, and 1485 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:54:30 : 1 steps taken, and 1433 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:55:01 : 5 steps taken, and 1461 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:55:19 : 8 steps taken, and 1489 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:55:32 : 8 steps taken, and 1436 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:55:33 : 9 steps taken, and 1486 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:56:09 : 13 steps taken, and 1461 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:56:26 : 15 steps taken, and 1490 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:56:33 : 16 steps taken, and 1488 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:56:40 : 13 steps taken, and 1437 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:57:21 : 23 steps taken, and 1462 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:57:30 : 24 steps taken, and 1491 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:57:37 : 23 steps taken, and 1490 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:57:43 : 19 steps taken, and 1439 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:58:25 : 31 steps taken, and 1464 accepted.
[2 : mcmc] Learn + convergence test @ 1440 samples accepted.
[2 : mcmc] Ready to check convergence and learn a new proposal covmat (waiting for the rest...)
[3 : mcmc] Progress @ 2025-02-21 14:58:37 : 31 steps taken, and 1493 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:58:37 : 33 steps taken, and 1494 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:58:47 : 27 steps taken, and 1440 accepted.
[3 : mcmc] Progress @ 2025-02-21 14:59:37 : 36 steps taken, and 1495 accepted.
[0 : mcmc] Progress @ 2025-02-21 14:59:38 : 40 steps taken, and 1465 accepted.
[2 : mcmc] Progress @ 2025-02-21 14:59:51 : 36 steps taken, and 1441 accepted.
[1 : mcmc] Progress @ 2025-02-21 14:59:51 : 39 steps taken, and 1495 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:00:39 : 49 steps taken, and 1466 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:00:41 : 43 steps taken, and 1496 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:00:51 : 42 steps taken, and 1443 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:00:52 : 45 steps taken, and 1497 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:01:39 : 54 steps taken, and 1466 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:01:53 : 51 steps taken, and 1500 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:01:55 : 49 steps taken, and 1497 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:02:05 : 50 steps taken, and 1444 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:02:52 : 62 steps taken, and 1468 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:03:01 : 57 steps taken, and 1497 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:03:07 : 59 steps taken, and 1503 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:03:09 : 56 steps taken, and 1444 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:03:56 : 70 steps taken, and 1471 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:04:11 : 62 steps taken, and 1498 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:04:12 : 62 steps taken, and 1447 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:04:14 : 67 steps taken, and 1506 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:05:10 : 80 steps taken, and 1473 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:05:14 : 73 steps taken, and 1507 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:05:15 : 70 steps taken, and 1501 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:05:26 : 72 steps taken, and 1450 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:06:11 : 85 steps taken, and 1475 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:06:18 : 80 steps taken, and 1509 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:06:26 : 79 steps taken, and 1452 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:06:28 : 79 steps taken, and 1506 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:07:14 : 94 steps taken, and 1479 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:07:19 : 87 steps taken, and 1509 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:07:26 : 87 steps taken, and 1453 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:07:34 : 86 steps taken, and 1506 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:08:18 : 101 steps taken, and 1482 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:08:20 : 92 steps taken, and 1510 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:08:30 : 94 steps taken, and 1457 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:08:48 : 95 steps taken, and 1506 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:09:19 : 108 steps taken, and 1484 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:09:20 : 99 steps taken, and 1511 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:09:42 : 106 steps taken, and 1460 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:10:00 : 104 steps taken, and 1507 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:10:19 : 115 steps taken, and 1485 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:10:23 : 105 steps taken, and 1513 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:10:43 : 117 steps taken, and 1461 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:11:01 : 113 steps taken, and 1509 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:11:20 : 123 steps taken, and 1487 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:11:35 : 113 steps taken, and 1513 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:11:55 : 130 steps taken, and 1464 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:12:11 : 123 steps taken, and 1511 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:12:31 : 132 steps taken, and 1488 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:12:43 : 119 steps taken, and 1514 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:13:02 : 140 steps taken, and 1468 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:13:12 : 134 steps taken, and 1514 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:13:33 : 139 steps taken, and 1489 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:13:54 : 127 steps taken, and 1517 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:14:10 : 149 steps taken, and 1471 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:14:16 : 150 steps taken, and 1515 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:14:44 : 147 steps taken, and 1489 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:14:55 : 133 steps taken, and 1519 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:15:11 : 159 steps taken, and 1472 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:15:16 : 159 steps taken, and 1516 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:15:44 : 157 steps taken, and 1490 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:16:06 : 144 steps taken, and 1520 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:16:12 : 169 steps taken, and 1473 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:16:26 : 167 steps taken, and 1519 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:16:45 : 164 steps taken, and 1493 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:17:13 : 175 steps taken, and 1474 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:17:17 : 152 steps taken, and 1522 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:17:37 : 174 steps taken, and 1519 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:17:50 : 171 steps taken, and 1495 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:18:23 : 159 steps taken, and 1522 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:18:24 : 181 steps taken, and 1475 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:18:38 : 181 steps taken, and 1521 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:18:58 : 178 steps taken, and 1496 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:19:26 : 187 steps taken, and 1475 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:19:35 : 166 steps taken, and 1523 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:19:39 : 187 steps taken, and 1522 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:20:05 : 184 steps taken, and 1497 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:20:27 : 197 steps taken, and 1476 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:20:46 : 195 steps taken, and 1524 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:20:47 : 172 steps taken, and 1525 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:21:06 : 190 steps taken, and 1498 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:21:38 : 205 steps taken, and 1476 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:21:48 : 178 steps taken, and 1525 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:21:54 : 203 steps taken, and 1527 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:22:17 : 197 steps taken, and 1499 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:22:49 : 213 steps taken, and 1478 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:22:57 : 184 steps taken, and 1526 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:23:05 : 213 steps taken, and 1530 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:23:18 : 205 steps taken, and 1499 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:23:50 : 220 steps taken, and 1481 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:24:04 : 192 steps taken, and 1527 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:24:05 : 221 steps taken, and 1530 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:24:35 : 212 steps taken, and 1499 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:25:01 : 226 steps taken, and 1482 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:25:06 : 227 steps taken, and 1531 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:25:15 : 199 steps taken, and 1528 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:25:36 : 221 steps taken, and 1500 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:26:03 : 233 steps taken, and 1483 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:26:17 : 235 steps taken, and 1532 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:26:17 : 205 steps taken, and 1531 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:26:36 : 228 steps taken, and 1501 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:27:04 : 240 steps taken, and 1485 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:27:18 : 242 steps taken, and 1533 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:27:25 : 210 steps taken, and 1532 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:27:40 : 234 steps taken, and 1502 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:28:11 : 247 steps taken, and 1485 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:28:19 : 251 steps taken, and 1537 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:28:27 : 217 steps taken, and 1535 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:28:49 : 241 steps taken, and 1503 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:29:22 : 255 steps taken, and 1487 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:29:30 : 262 steps taken, and 1540 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:29:38 : 224 steps taken, and 1535 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:29:50 : 247 steps taken, and 1507 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:30:27 : 264 steps taken, and 1491 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:30:31 : 269 steps taken, and 1540 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:30:50 : 230 steps taken, and 1537 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:30:59 : 254 steps taken, and 1508 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:31:32 : 275 steps taken, and 1540 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:31:38 : 271 steps taken, and 1491 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:31:55 : 238 steps taken, and 1539 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:32:00 : 262 steps taken, and 1510 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:32:44 : 284 steps taken, and 1541 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:32:50 : 279 steps taken, and 1494 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:33:01 : 269 steps taken, and 1511 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:33:04 : 243 steps taken, and 1540 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:33:55 : 290 steps taken, and 1543 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:33:58 : 284 steps taken, and 1495 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:34:05 : 251 steps taken, and 1542 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:34:13 : 276 steps taken, and 1514 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:34:56 : 301 steps taken, and 1545 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:34:59 : 290 steps taken, and 1497 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:35:13 : 256 steps taken, and 1542 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:35:14 : 282 steps taken, and 1515 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:36:00 : 297 steps taken, and 1498 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:36:07 : 310 steps taken, and 1547 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:36:14 : 265 steps taken, and 1542 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:36:16 : 292 steps taken, and 1516 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:37:11 : 319 steps taken, and 1550 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:37:11 : 310 steps taken, and 1500 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:37:15 : 277 steps taken, and 1547 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:37:27 : 300 steps taken, and 1518 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:38:11 : 325 steps taken, and 1552 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:38:12 : 321 steps taken, and 1501 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:38:16 : 286 steps taken, and 1548 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:38:27 : 310 steps taken, and 1521 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:39:14 : 332 steps taken, and 1503 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:39:23 : 332 steps taken, and 1553 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:39:27 : 296 steps taken, and 1551 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:39:35 : 316 steps taken, and 1522 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:40:15 : 338 steps taken, and 1504 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:40:23 : 339 steps taken, and 1554 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:40:35 : 302 steps taken, and 1554 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:40:47 : 326 steps taken, and 1523 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:41:15 : 349 steps taken, and 1507 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:41:30 : 345 steps taken, and 1558 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:41:36 : 309 steps taken, and 1557 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:41:59 : 335 steps taken, and 1525 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:42:16 : 357 steps taken, and 1509 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:42:30 : 352 steps taken, and 1560 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:42:48 : 316 steps taken, and 1558 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:43:10 : 343 steps taken, and 1528 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:43:24 : 362 steps taken, and 1509 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:43:31 : 360 steps taken, and 1564 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:43:59 : 325 steps taken, and 1558 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:44:11 : 352 steps taken, and 1529 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:44:28 : 375 steps taken, and 1512 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:44:42 : 367 steps taken, and 1566 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:45:01 : 331 steps taken, and 1561 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:45:12 : 359 steps taken, and 1529 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:45:32 : 387 steps taken, and 1514 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:45:53 : 373 steps taken, and 1567 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:46:08 : 336 steps taken, and 1562 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:46:20 : 368 steps taken, and 1531 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:46:33 : 394 steps taken, and 1515 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:47:00 : 384 steps taken, and 1568 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:47:10 : 343 steps taken, and 1562 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:47:21 : 381 steps taken, and 1532 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:47:34 : 400 steps taken, and 1515 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:48:08 : 389 steps taken, and 1569 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:48:11 : 349 steps taken, and 1563 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:48:32 : 388 steps taken, and 1535 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:48:42 : 406 steps taken, and 1517 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:49:08 : 396 steps taken, and 1571 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:49:19 : 354 steps taken, and 1565 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:49:43 : 394 steps taken, and 1536 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:49:44 : 412 steps taken, and 1517 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:50:09 : 406 steps taken, and 1574 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:50:24 : 362 steps taken, and 1565 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:50:44 : 401 steps taken, and 1537 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:50:45 : 423 steps taken, and 1520 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:51:20 : 413 steps taken, and 1576 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:51:36 : 368 steps taken, and 1567 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:51:47 : 432 steps taken, and 1523 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:51:55 : 408 steps taken, and 1537 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:52:31 : 420 steps taken, and 1578 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:52:47 : 376 steps taken, and 1567 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:52:48 : 438 steps taken, and 1524 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:52:56 : 414 steps taken, and 1537 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:53:42 : 426 steps taken, and 1580 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:53:48 : 385 steps taken, and 1571 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:53:49 : 444 steps taken, and 1525 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:53:57 : 420 steps taken, and 1538 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:54:52 : 435 steps taken, and 1581 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:54:58 : 451 steps taken, and 1525 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:54:59 : 391 steps taken, and 1573 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:55:09 : 428 steps taken, and 1538 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:55:52 : 442 steps taken, and 1583 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:55:59 : 458 steps taken, and 1527 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:56:04 : 400 steps taken, and 1576 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:56:11 : 437 steps taken, and 1540 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:57:00 : 464 steps taken, and 1528 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:57:03 : 449 steps taken, and 1585 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:57:16 : 411 steps taken, and 1578 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:57:23 : 443 steps taken, and 1542 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:58:08 : 470 steps taken, and 1529 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:58:10 : 454 steps taken, and 1586 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:58:17 : 420 steps taken, and 1579 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:58:24 : 450 steps taken, and 1542 accepted.
[2 : mcmc] Progress @ 2025-02-21 15:59:09 : 477 steps taken, and 1530 accepted.
[1 : mcmc] Progress @ 2025-02-21 15:59:19 : 429 steps taken, and 1580 accepted.
[3 : mcmc] Progress @ 2025-02-21 15:59:21 : 461 steps taken, and 1587 accepted.
[0 : mcmc] Progress @ 2025-02-21 15:59:36 : 457 steps taken, and 1544 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:00:10 : 484 steps taken, and 1531 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:00:21 : 467 steps taken, and 1588 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:00:27 : 435 steps taken, and 1582 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:00:47 : 465 steps taken, and 1546 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:01:21 : 490 steps taken, and 1534 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:01:22 : 474 steps taken, and 1589 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:01:38 : 444 steps taken, and 1584 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:01:58 : 473 steps taken, and 1547 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:02:33 : 496 steps taken, and 1536 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:02:34 : 480 steps taken, and 1592 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:02:40 : 450 steps taken, and 1588 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:02:59 : 480 steps taken, and 1549 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:03:33 : 503 steps taken, and 1538 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:03:35 : 488 steps taken, and 1594 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:03:41 : 460 steps taken, and 1589 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:04:03 : 490 steps taken, and 1550 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:04:34 : 518 steps taken, and 1538 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:04:47 : 495 steps taken, and 1594 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:04:54 : 467 steps taken, and 1590 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:05:04 : 502 steps taken, and 1553 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:05:35 : 525 steps taken, and 1541 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:05:55 : 473 steps taken, and 1591 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:05:58 : 501 steps taken, and 1595 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:06:15 : 512 steps taken, and 1556 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:06:45 : 533 steps taken, and 1542 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:07:07 : 479 steps taken, and 1594 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:07:09 : 507 steps taken, and 1598 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:07:21 : 520 steps taken, and 1557 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:07:57 : 539 steps taken, and 1544 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:08:08 : 488 steps taken, and 1595 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:08:19 : 515 steps taken, and 1599 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:08:23 : 527 steps taken, and 1558 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:08:58 : 545 steps taken, and 1547 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:09:17 : 494 steps taken, and 1595 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:09:30 : 521 steps taken, and 1600 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:09:31 : 533 steps taken, and 1558 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:10:09 : 551 steps taken, and 1550 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:10:22 : 506 steps taken, and 1596 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:10:30 : 529 steps taken, and 1603 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:10:42 : 541 steps taken, and 1561 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:11:11 : 557 steps taken, and 1552 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:11:23 : 517 steps taken, and 1599 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:11:31 : 536 steps taken, and 1604 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:11:53 : 554 steps taken, and 1562 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:12:12 : 564 steps taken, and 1553 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:12:24 : 524 steps taken, and 1600 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:12:39 : 544 steps taken, and 1607 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:12:54 : 564 steps taken, and 1564 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:13:20 : 569 steps taken, and 1556 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:13:35 : 535 steps taken, and 1601 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:13:40 : 551 steps taken, and 1608 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:14:05 : 571 steps taken, and 1566 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:14:21 : 577 steps taken, and 1557 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:14:40 : 562 steps taken, and 1610 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:14:46 : 543 steps taken, and 1604 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:15:06 : 577 steps taken, and 1569 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:15:32 : 585 steps taken, and 1558 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:15:40 : 571 steps taken, and 1611 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:15:59 : 554 steps taken, and 1606 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:16:15 : 587 steps taken, and 1571 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:16:33 : 592 steps taken, and 1560 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:16:51 : 578 steps taken, and 1612 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:17:11 : 562 steps taken, and 1608 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:17:17 : 593 steps taken, and 1574 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:17:34 : 598 steps taken, and 1560 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:17:53 : 587 steps taken, and 1612 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:18:11 : 575 steps taken, and 1608 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:18:17 : 602 steps taken, and 1575 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:18:41 : 604 steps taken, and 1561 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:18:58 : 598 steps taken, and 1614 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:19:23 : 581 steps taken, and 1609 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:19:28 : 609 steps taken, and 1576 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:19:49 : 611 steps taken, and 1563 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:20:08 : 605 steps taken, and 1615 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:20:25 : 589 steps taken, and 1610 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:20:34 : 617 steps taken, and 1576 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:20:50 : 617 steps taken, and 1565 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:21:10 : 615 steps taken, and 1615 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:21:35 : 623 steps taken, and 1576 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:21:36 : 599 steps taken, and 1613 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:21:57 : 623 steps taken, and 1565 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:22:11 : 625 steps taken, and 1618 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:22:35 : 634 steps taken, and 1577 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:22:48 : 606 steps taken, and 1613 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:23:03 : 631 steps taken, and 1567 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:23:17 : 635 steps taken, and 1620 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:23:42 : 639 steps taken, and 1577 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:23:50 : 613 steps taken, and 1613 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:24:15 : 638 steps taken, and 1570 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:24:19 : 642 steps taken, and 1622 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:24:47 : 646 steps taken, and 1579 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:24:51 : 620 steps taken, and 1614 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:25:16 : 645 steps taken, and 1571 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:25:19 : 653 steps taken, and 1623 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:25:53 : 627 steps taken, and 1616 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:25:58 : 656 steps taken, and 1581 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:26:17 : 651 steps taken, and 1573 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:26:20 : 660 steps taken, and 1624 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:26:56 : 636 steps taken, and 1618 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:27:00 : 663 steps taken, and 1581 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:27:28 : 657 steps taken, and 1575 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:27:31 : 670 steps taken, and 1624 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:28:04 : 642 steps taken, and 1618 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:28:13 : 670 steps taken, and 1581 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:28:35 : 663 steps taken, and 1576 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:28:39 : 683 steps taken, and 1624 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:29:06 : 650 steps taken, and 1619 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:29:14 : 677 steps taken, and 1582 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:29:47 : 669 steps taken, and 1580 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:29:50 : 689 steps taken, and 1625 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:30:07 : 658 steps taken, and 1620 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:30:20 : 683 steps taken, and 1584 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:30:47 : 675 steps taken, and 1580 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:31:01 : 695 steps taken, and 1626 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:31:08 : 665 steps taken, and 1621 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:31:27 : 688 steps taken, and 1584 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:31:48 : 682 steps taken, and 1581 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:32:03 : 702 steps taken, and 1629 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:32:17 : 671 steps taken, and 1621 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:32:28 : 695 steps taken, and 1586 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:32:59 : 689 steps taken, and 1582 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:33:04 : 708 steps taken, and 1631 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:33:25 : 680 steps taken, and 1622 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:33:30 : 701 steps taken, and 1588 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:34:00 : 695 steps taken, and 1583 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:34:12 : 713 steps taken, and 1632 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:34:26 : 691 steps taken, and 1624 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:34:31 : 708 steps taken, and 1588 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:35:00 : 710 steps taken, and 1585 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:35:12 : 720 steps taken, and 1634 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:35:37 : 714 steps taken, and 1588 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:35:38 : 698 steps taken, and 1624 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:36:04 : 721 steps taken, and 1586 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:36:12 : 728 steps taken, and 1638 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:36:43 : 709 steps taken, and 1625 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:36:48 : 721 steps taken, and 1591 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:37:06 : 729 steps taken, and 1590 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:37:12 : 742 steps taken, and 1639 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:37:44 : 718 steps taken, and 1627 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:38:00 : 727 steps taken, and 1593 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:38:13 : 750 steps taken, and 1641 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:38:18 : 736 steps taken, and 1592 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:38:45 : 725 steps taken, and 1629 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:39:01 : 734 steps taken, and 1594 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:39:13 : 759 steps taken, and 1643 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:39:20 : 744 steps taken, and 1593 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:39:46 : 732 steps taken, and 1630 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:40:02 : 742 steps taken, and 1597 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:40:25 : 768 steps taken, and 1643 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:40:33 : 753 steps taken, and 1594 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:40:55 : 738 steps taken, and 1631 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:41:15 : 749 steps taken, and 1599 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:41:32 : 777 steps taken, and 1643 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:41:35 : 761 steps taken, and 1598 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:41:56 : 745 steps taken, and 1631 accepted.
[node147:55224:0:55224] Caught signal 11 (Segmentation fault: address not mapped to object at address 0xfffffffc04a733d8)
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
==== backtrace (tid:  55224) ====
 0 0x0000000000021563 ucs_debug_print_backtrace()  /dev/shm/UCX/1.8.0/GCCcore-9.3.0/ucx-1.8.0/src/ucs/debug/debug.c:653
 1 0x0000000000039f63 __massivenu_MOD_thermalnubackground_rho()  ???:0
 2 0x000000000003da49 __results_MOD_grho_no_de()  ???:0
 3 0x00000000000a5f01 dtauda_()  ???:0
 4 0x000000000004cfdc __results_MOD_thermo_init._omp_fn.0()  results.f90:0
 5 0x0000000000014295 GOMP_parallel_sections()  ???:0
 6 0x000000000004f7c2 __results_MOD_thermo_init()  ???:0
 7 0x00000000000f3923 __cambmain_MOD_initvars()  ???:0
 8 0x00000000000f9bff __cambmain_MOD_cmbmain()  ???:0
 9 0x0000000000105846 __camb_MOD_camb_getresults()  ???:0
10 0x00000000001104df __handles_MOD_cambdata_gettransfers()  ???:0
11 0x0000000000006a4a ffi_call_unix64()  :0
12 0x0000000000005fea ffi_call_int()  ffi64.c:0
13 0x00000000000091e0 _ctypes_callproc.cold()  :0
14 0x00000000000126ee PyCFuncPtr_call()  :0
15 0x00000000004f061c _PyObject_MakeTpCall()  ???:0
16 0x00000000004ec4f7 _PyEval_EvalFrameDefault()  ???:0
17 0x00000000004e694a _PyEval_EvalCode()  :0
18 0x000000000050500d method_vectorcall()  :0
19 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
20 0x00000000004e694a _PyEval_EvalCode()  :0
21 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
22 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
23 0x00000000004e694a _PyEval_EvalCode()  :0
24 0x000000000050500d method_vectorcall()  :0
25 0x0000000000505744 PyObject_Call()  ???:0
26 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
27 0x00000000004e694a _PyEval_EvalCode()  :0
28 0x000000000050500d method_vectorcall()  :0
29 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
30 0x00000000004e694a _PyEval_EvalCode()  :0
31 0x000000000050500d method_vectorcall()  :0
32 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
33 0x00000000004e694a _PyEval_EvalCode()  :0
34 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
35 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
36 0x00000000004f8073 function_code_fastcall()  :0
37 0x0000000000504f01 method_vectorcall()  :0
38 0x00000000004ec5b4 _PyEval_EvalFrameDefault()  ???:0
39 0x00000000004f8073 function_code_fastcall()  :0
40 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
41 0x00000000004e694a _PyEval_EvalCode()  :0
42 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
43 0x0000000000505744 PyObject_Call()  ???:0
44 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
45 0x00000000004f8073 function_code_fastcall()  :0
46 0x00000000004e7c22 _PyEval_EvalFrameDefault()  ???:0
47 0x00000000004e694a _PyEval_EvalCode()  :0
48 0x00000000004e65d7 _PyEval_EvalCodeWithName()  ???:0
49 0x00000000004e6589 PyEval_EvalCodeEx()  ???:0
50 0x0000000000593f2b PyEval_EvalCode()  ???:0
51 0x00000000005c1697 run_eval_code_obj()  :0
52 0x00000000005bd6b0 run_mod()  :0
53 0x00000000004565eb pyrun_file.cold()  :0
54 0x00000000005b7392 PyRun_SimpleFileExFlags()  ???:0
55 0x00000000005b490e Py_RunMain()  ???:0
56 0x0000000000587fd9 Py_BytesMain()  ???:0
57 0x0000000000022555 __libc_start_main()  ???:0
=================================
[node147:55224] *** Process received signal ***
[node147:55224] Signal: Segmentation fault (11)
[node147:55224] Signal code:  (-6)
[node147:55224] Failing at address: 0x8cfb80000d7b8
[node147:55224] [ 0] /lib64/libpthread.so.0(+0xf630)[0x7ff8ae231630]
[node147:55224] [ 1] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__massivenu_MOD_thermalnubackground_rho+0x63)[0x7ff88cf31f63]
[node147:55224] [ 2] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_grho_no_de+0xc9)[0x7ff88cf35a49]
[node147:55224] [ 3] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(dtauda_+0x61)[0x7ff88cf9df01]
[node147:55224] [ 4] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(+0x4cfdc)[0x7ff88cf44fdc]
[node147:55224] [ 5] /users/smp24dhl/.conda/envs/cosmos/bin/../lib/libgomp.so.1(GOMP_parallel_sections+0x80)[0x7ff88cecd295]
[node147:55224] [ 6] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_thermo_init+0x12d2)[0x7ff88cf477c2]
[node147:55224] [ 7] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_initvars+0x193)[0x7ff88cfeb923]
[node147:55224] [ 8] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_cmbmain+0xbf)[0x7ff88cff1bff]
[node147:55224] [ 9] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__camb_MOD_camb_getresults+0x2986)[0x7ff88cffd846]
[node147:55224] [10] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__handles_MOD_cambdata_gettransfers+0x1f)[0x7ff88d0084df]
[node147:55224] [11] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x6a4a)[0x7ff8ae4eca4a]
[node147:55224] [12] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x5fea)[0x7ff8ae4ebfea]
[node147:55224] [13] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x91e0)[0x7ff8a41eb1e0]
[node147:55224] [14] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x126ee)[0x7ff8a41f46ee]
[node147:55224] [15] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyObject_MakeTpCall+0x2ec)[0x4f061c]
[node147:55224] [16] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x4ca7)[0x4ec4f7]
[node147:55224] [17] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:55224] [18] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:55224] [19] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:55224] [20] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:55224] [21] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyFunction_Vectorcall+0xd5)[0x4f7d95]
[node147:55224] [22] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:55224] [23] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:55224] [24] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:55224] [25] /users/smp24dhl/.conda/envs/cosmos/bin/python(PyObject_Call+0xb4)[0x505744]
[node147:55224] [26] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x3418)[0x4eac68]
[node147:55224] [27] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:55224] [28] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:55224] [29] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:55224] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 2 with PID 55224 on node node147 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
./chains/PlanckLensingDESI.1.txt
./chains/PlanckLensingDESI.4.txt
./chains/PlanckLensingDESI.2.txt
./chains/PlanckLensingDESI.3.txt
Removed 0.3 as burn in
R-1 = 1.159872945430334
H_0 = 66.3^{+2.8}_{-3.2}
w_{0,\mathrm{DE}} = -0.59^{+0.30}_{-0.26}
w_{a,\mathrm{DE}} < -1.46
\beta_{\mathrm{DE}} = 2.5^{+1.0}_{-2.1}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] Found an old sample. Resuming.
[0 : prior] *WARNING* External prior 'SZ' loaded. Mind that it might not be normalized!
[0 : camb] `camb` module loaded successfully from /users/smp24dhl/cosmo/code/CAMB_beta/camb
[0 : planck_2018_highl_plik.ttteee] `clik` module loaded successfully from /users/smp24dhl/cosmo/code/planck/clik/lib/python3.9/site-packages/clik
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[2 : bao.desi_2024_bao_all] Initialized.
[0 : bao.desi_2024_bao_all] Initialized.
[0 : mcmc] Resuming from previous sample!
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[3 : bao.desi_2024_bao_all] Initialized.
[1 : bao.desi_2024_bao_all] Initialized.
[0 : samplecollection] Loaded 1599 sample points from 'chains/PlanckLensingDESI.1.txt'
[3 : samplecollection] Loaded 1643 sample points from 'chains/PlanckLensingDESI.4.txt'
[2 : samplecollection] Loaded 1598 sample points from 'chains/PlanckLensingDESI.3.txt'
[1 : samplecollection] Loaded 1631 sample points from 'chains/PlanckLensingDESI.2.txt'
[3 : mcmc] Initial point: logA:3.024982, ns:0.9704084, theta_MC_100:1.040699, ombh2:0.02228401, omch2:0.1198074, w:-0.450025, wa:-2.116757, beta_DE:2.176169, tau:0.04046357, A_planck:1.005445, calib_100T:0.9986483, calib_217T:0.9980798, A_cib_217:43.12617, xi_sz_cib:0.915728, A_sz:6.430726, ksz_norm:3.255961, gal545_A_100:6.567667, gal545_A_143:10.70507, gal545_A_143_217:19.93939, gal545_A_217:99.23221, ps_A_100_100:247.3968, ps_A_143_143:43.84231, ps_A_143_217:51.62669, ps_A_217_217:119.0088, galf_TE_A_100:0.1724916, galf_TE_A_100_143:0.144774, galf_TE_A_100_217:0.5540616, galf_TE_A_143:0.2542193, galf_TE_A_143_217:0.7308233, galf_TE_A_217:2.342905
[2 : mcmc] Initial point: logA:3.033867, ns:0.9641004, theta_MC_100:1.040598, ombh2:0.02240277, omch2:0.1189881, w:-0.49862, wa:-1.257148, beta_DE:0.0862564, tau:0.0464796, A_planck:1.004672, calib_100T:1.000201, calib_217T:0.9986293, A_cib_217:42.35892, xi_sz_cib:0.4999689, A_sz:3.042871, ksz_norm:8.176715, gal545_A_100:11.0145, gal545_A_143:9.29241, gal545_A_143_217:14.98873, gal545_A_217:93.02642, ps_A_100_100:240.3457, ps_A_143_143:47.31229, ps_A_143_217:37.07278, ps_A_217_217:116.3914, galf_TE_A_100:0.0917945, galf_TE_A_100_143:0.09034398, galf_TE_A_100_217:0.4442586, galf_TE_A_143:0.2528424, galf_TE_A_143_217:0.5826786, galf_TE_A_217:1.763585
[0 : mcmc] Initial point: logA:3.059932, ns:0.9699363, theta_MC_100:1.040597, ombh2:0.02221445, omch2:0.1189378, w:-0.6787393, wa:-1.078599, beta_DE:0.872574, tau:0.06362972, A_planck:1.001003, calib_100T:0.9989075, calib_217T:0.9975636, A_cib_217:45.07347, xi_sz_cib:0.6119869, A_sz:5.217528, ksz_norm:3.743506, gal545_A_100:9.748221, gal545_A_143:13.45292, gal545_A_143_217:18.47279, gal545_A_217:90.62449, ps_A_100_100:247.9848, ps_A_143_143:38.06423, ps_A_143_217:40.23376, ps_A_217_217:119.6253, galf_TE_A_100:0.106626, galf_TE_A_100_143:0.1573742, galf_TE_A_100_217:0.3996259, galf_TE_A_143:0.274395, galf_TE_A_143_217:0.6730139, galf_TE_A_217:1.911647
[1 : mcmc] Initial point: logA:3.038919, ns:0.9681749, theta_MC_100:1.041055, ombh2:0.02234649, omch2:0.1192229, w:-0.8103001, wa:-1.720587, beta_DE:4.0284, tau:0.0526109, A_planck:1.000116, calib_100T:1.000867, calib_217T:0.997983, A_cib_217:45.01355, xi_sz_cib:0.2394975, A_sz:5.006195, ksz_norm:6.111997, gal545_A_100:4.928872, gal545_A_143:7.678235, gal545_A_143_217:14.29851, gal545_A_217:90.60905, ps_A_100_100:291.7279, ps_A_143_143:44.23409, ps_A_143_217:33.96228, ps_A_217_217:113.4721, galf_TE_A_100:0.05045235, galf_TE_A_100_143:0.1474478, galf_TE_A_100_217:0.5842158, galf_TE_A_143:0.256678, galf_TE_A_143_217:0.619032, galf_TE_A_217:1.768546
[0 : mcmc] *WARNING* Parameter blocking manually/previously fixed: speeds will not be measured.
[0 : mcmc] Dragging with number of interpolating steps:
[0 : mcmc] *  1 : (['theta_MC_100', 'ombh2', 'omch2', 'w', 'wa', 'beta_DE', 'tau'], ['logA', 'ns'])
[0 : mcmc] * 12 : (['A_planck'], ['calib_100T', 'calib_217T', 'A_cib_217', 'xi_sz_cib', 'A_sz', 'ksz_norm', 'gal545_A_100', 'gal545_A_143', 'gal545_A_143_217', 'gal545_A_217', 'ps_A_100_100', 'ps_A_143_143', 'ps_A_143_217', 'ps_A_217_217', 'galf_TE_A_100', 'galf_TE_A_100_143', 'galf_TE_A_100_217', 'galf_TE_A_143', 'galf_TE_A_143_217', 'galf_TE_A_217'])
[0 : mcmc] Covariance matrix from previous sample.
[0 : mcmc] Sampling!
[2 : mcmc] Progress @ 2025-02-21 16:42:22 : 1 steps taken, and 1598 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:42:51 : 1 steps taken, and 1643 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:42:51 : 1 steps taken, and 1599 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:42:52 : 1 steps taken, and 1631 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:43:26 : 6 steps taken, and 1598 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:43:51 : 11 steps taken, and 1643 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:43:53 : 10 steps taken, and 1599 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:44:03 : 7 steps taken, and 1631 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:44:28 : 15 steps taken, and 1601 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:44:55 : 17 steps taken, and 1601 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:44:59 : 25 steps taken, and 1645 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:45:05 : 13 steps taken, and 1633 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:45:38 : 21 steps taken, and 1601 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:46:00 : 37 steps taken, and 1647 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:46:01 : 24 steps taken, and 1602 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:46:16 : 19 steps taken, and 1634 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:46:50 : 29 steps taken, and 1603 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:47:03 : 31 steps taken, and 1602 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:47:04 : 48 steps taken, and 1647 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:47:28 : 27 steps taken, and 1634 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:48:01 : 45 steps taken, and 1604 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:48:05 : 56 steps taken, and 1649 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:48:12 : 36 steps taken, and 1602 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:48:36 : 32 steps taken, and 1636 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:49:05 : 59 steps taken, and 1607 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:49:06 : 65 steps taken, and 1652 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:49:21 : 44 steps taken, and 1603 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:49:37 : 39 steps taken, and 1637 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:50:13 : 65 steps taken, and 1608 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:50:17 : 71 steps taken, and 1652 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:50:33 : 53 steps taken, and 1605 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:50:39 : 46 steps taken, and 1639 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:51:24 : 72 steps taken, and 1610 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:51:26 : 77 steps taken, and 1654 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:51:37 : 60 steps taken, and 1606 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:51:43 : 58 steps taken, and 1641 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:52:25 : 79 steps taken, and 1612 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:52:27 : 83 steps taken, and 1656 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:52:38 : 67 steps taken, and 1607 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:52:55 : 65 steps taken, and 1644 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:53:27 : 85 steps taken, and 1613 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:53:28 : 89 steps taken, and 1657 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:53:47 : 74 steps taken, and 1609 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:53:56 : 76 steps taken, and 1645 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:54:28 : 96 steps taken, and 1660 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:54:28 : 93 steps taken, and 1615 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:54:49 : 81 steps taken, and 1610 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:55:07 : 82 steps taken, and 1647 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:55:29 : 99 steps taken, and 1615 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:55:39 : 103 steps taken, and 1661 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:55:58 : 89 steps taken, and 1612 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:56:09 : 90 steps taken, and 1647 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:56:31 : 107 steps taken, and 1616 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:56:47 : 111 steps taken, and 1662 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:57:07 : 94 steps taken, and 1613 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:57:10 : 97 steps taken, and 1650 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:57:39 : 112 steps taken, and 1619 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:57:57 : 121 steps taken, and 1665 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:58:19 : 104 steps taken, and 1651 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:58:19 : 106 steps taken, and 1614 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:58:40 : 119 steps taken, and 1619 accepted.
[3 : mcmc] Progress @ 2025-02-21 16:59:07 : 130 steps taken, and 1666 accepted.
[1 : mcmc] Progress @ 2025-02-21 16:59:27 : 119 steps taken, and 1654 accepted.
[0 : mcmc] Progress @ 2025-02-21 16:59:30 : 113 steps taken, and 1615 accepted.
[2 : mcmc] Progress @ 2025-02-21 16:59:42 : 128 steps taken, and 1620 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:00:14 : 140 steps taken, and 1668 accepted.
[node147:63489:0:63489] Caught signal 11 (Segmentation fault: address not mapped to object at address 0xfffffffc0600f018)
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
==== backtrace (tid:  63489) ====
 0 0x0000000000021563 ucs_debug_print_backtrace()  /dev/shm/UCX/1.8.0/GCCcore-9.3.0/ucx-1.8.0/src/ucs/debug/debug.c:653
 1 0x0000000000039f63 __massivenu_MOD_thermalnubackground_rho()  ???:0
 2 0x000000000003da49 __results_MOD_grho_no_de()  ???:0
 3 0x00000000000a5f01 dtauda_()  ???:0
 4 0x000000000004cfdc __results_MOD_thermo_init._omp_fn.0()  results.f90:0
 5 0x0000000000014295 GOMP_parallel_sections()  ???:0
 6 0x000000000004f7c2 __results_MOD_thermo_init()  ???:0
 7 0x00000000000f3923 __cambmain_MOD_initvars()  ???:0
 8 0x00000000000f9bff __cambmain_MOD_cmbmain()  ???:0
 9 0x0000000000105846 __camb_MOD_camb_getresults()  ???:0
10 0x00000000001104df __handles_MOD_cambdata_gettransfers()  ???:0
11 0x0000000000006a4a ffi_call_unix64()  :0
12 0x0000000000005fea ffi_call_int()  ffi64.c:0
13 0x00000000000091e0 _ctypes_callproc.cold()  :0
14 0x00000000000126ee PyCFuncPtr_call()  :0
15 0x00000000004f061c _PyObject_MakeTpCall()  ???:0
16 0x00000000004ec4f7 _PyEval_EvalFrameDefault()  ???:0
17 0x00000000004e694a _PyEval_EvalCode()  :0
18 0x000000000050500d method_vectorcall()  :0
19 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
20 0x00000000004e694a _PyEval_EvalCode()  :0
21 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
22 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
23 0x00000000004e694a _PyEval_EvalCode()  :0
24 0x000000000050500d method_vectorcall()  :0
25 0x0000000000505744 PyObject_Call()  ???:0
26 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
27 0x00000000004e694a _PyEval_EvalCode()  :0
28 0x000000000050500d method_vectorcall()  :0
29 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
30 0x00000000004e694a _PyEval_EvalCode()  :0
31 0x000000000050500d method_vectorcall()  :0
32 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
33 0x00000000004e694a _PyEval_EvalCode()  :0
34 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
35 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
36 0x00000000004f8073 function_code_fastcall()  :0
37 0x0000000000504f01 method_vectorcall()  :0
38 0x00000000004ec5b4 _PyEval_EvalFrameDefault()  ???:0
39 0x00000000004f8073 function_code_fastcall()  :0
40 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
41 0x00000000004e694a _PyEval_EvalCode()  :0
42 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
43 0x0000000000505744 PyObject_Call()  ???:0
44 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
45 0x00000000004f8073 function_code_fastcall()  :0
46 0x00000000004e7c22 _PyEval_EvalFrameDefault()  ???:0
47 0x00000000004e694a _PyEval_EvalCode()  :0
48 0x00000000004e65d7 _PyEval_EvalCodeWithName()  ???:0
49 0x00000000004e6589 PyEval_EvalCodeEx()  ???:0
50 0x0000000000593f2b PyEval_EvalCode()  ???:0
51 0x00000000005c1697 run_eval_code_obj()  :0
52 0x00000000005bd6b0 run_mod()  :0
53 0x00000000004565eb pyrun_file.cold()  :0
54 0x00000000005b7392 PyRun_SimpleFileExFlags()  ???:0
55 0x00000000005b490e Py_RunMain()  ???:0
56 0x0000000000587fd9 Py_BytesMain()  ???:0
57 0x0000000000022555 __libc_start_main()  ???:0
=================================
[node147:63489] *** Process received signal ***
[node147:63489] Signal: Segmentation fault (11)
[node147:63489] Signal code:  (-6)
[node147:63489] Failing at address: 0x8cfb80000f801
[node147:63489] [ 0] /lib64/libpthread.so.0(+0xf630)[0x7f313a0ff630]
[node147:63489] [ 1] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__massivenu_MOD_thermalnubackground_rho+0x63)[0x7f31189f5f63]
[node147:63489] [ 2] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_grho_no_de+0xc9)[0x7f31189f9a49]
[node147:63489] [ 3] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(dtauda_+0x61)[0x7f3118a61f01]
[node147:63489] [ 4] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(+0x4cfdc)[0x7f3118a08fdc]
[node147:63489] [ 5] /users/smp24dhl/.conda/envs/cosmos/bin/../lib/libgomp.so.1(GOMP_parallel_sections+0x80)[0x7f3118991295]
[node147:63489] [ 6] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_thermo_init+0x12d2)[0x7f3118a0b7c2]
[node147:63489] [ 7] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_initvars+0x193)[0x7f3118aaf923]
[node147:63489] [ 8] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_cmbmain+0xbf)[0x7f3118ab5bff]
[node147:63489] [ 9] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__camb_MOD_camb_getresults+0x2986)[0x7f3118ac1846]
[node147:63489] [10] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__handles_MOD_cambdata_gettransfers+0x1f)[0x7f3118acc4df]
[node147:63489] [11] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x6a4a)[0x7f313a3baa4a]
[node147:63489] [12] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x5fea)[0x7f313a3b9fea]
[node147:63489] [13] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x91e0)[0x7f31300b91e0]
[node147:63489] [14] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x126ee)[0x7f31300c26ee]
[node147:63489] [15] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyObject_MakeTpCall+0x2ec)[0x4f061c]
[node147:63489] [16] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x4ca7)[0x4ec4f7]
[node147:63489] [17] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:63489] [18] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:63489] [19] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:63489] [20] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:63489] [21] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyFunction_Vectorcall+0xd5)[0x4f7d95]
[node147:63489] [22] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:63489] [23] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:63489] [24] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:63489] [25] /users/smp24dhl/.conda/envs/cosmos/bin/python(PyObject_Call+0xb4)[0x505744]
[node147:63489] [26] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x3418)[0x4eac68]
[node147:63489] [27] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:63489] [28] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:63489] [29] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:63489] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 63489 on node node147 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
./chains/PlanckLensingDESI.1.txt
./chains/PlanckLensingDESI.4.txt
./chains/PlanckLensingDESI.2.txt
./chains/PlanckLensingDESI.3.txt
Removed 0.3 as burn in
R-1 = 1.0641162565104192
H_0 = 66.3^{+2.8}_{-3.2}
w_{0,\mathrm{DE}} = -0.59^{+0.30}_{-0.25}
w_{a,\mathrm{DE}} < -1.45
\beta_{\mathrm{DE}} = 2.5^{+1.1}_{-2.1}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] Found an old sample. Resuming.
[0 : prior] *WARNING* External prior 'SZ' loaded. Mind that it might not be normalized!
[0 : camb] `camb` module loaded successfully from /users/smp24dhl/cosmo/code/CAMB_beta/camb
[0 : planck_2018_highl_plik.ttteee] `clik` module loaded successfully from /users/smp24dhl/cosmo/code/planck/clik/lib/python3.9/site-packages/clik
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
  smica
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[0 : bao.desi_2024_bao_all] Initialized.
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[2 : bao.desi_2024_bao_all] Initialized.
[0 : mcmc] Resuming from previous sample!
[3 : bao.desi_2024_bao_all] Initialized.
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[1 : bao.desi_2024_bao_all] Initialized.
[0 : samplecollection] Loaded 1615 sample points from 'chains/PlanckLensingDESI.1.txt'
[2 : samplecollection] Loaded 1620 sample points from 'chains/PlanckLensingDESI.3.txt'
[1 : samplecollection] Loaded 1657 sample points from 'chains/PlanckLensingDESI.2.txt'
[3 : samplecollection] Loaded 1668 sample points from 'chains/PlanckLensingDESI.4.txt'
[0 : mcmc] Initial point: logA:3.028098, ns:0.9690303, theta_MC_100:1.041301, ombh2:0.02226672, omch2:0.1208704, w:-0.3732047, wa:-1.475735, beta_DE:-0.3600458, tau:0.04938831, A_planck:0.9964609, calib_100T:0.9994903, calib_217T:0.9974738, A_cib_217:46.4813, xi_sz_cib:0.4350869, A_sz:8.561248, ksz_norm:0.06355711, gal545_A_100:8.071165, gal545_A_143:10.7977, gal545_A_143_217:19.24177, gal545_A_217:91.81579, ps_A_100_100:206.501, ps_A_143_143:38.45527, ps_A_143_217:45.4098, ps_A_217_217:115.9309, galf_TE_A_100:0.1273199, galf_TE_A_100_143:0.1288909, galf_TE_A_100_217:0.2882076, galf_TE_A_143:0.2100431, galf_TE_A_143_217:0.6114391, galf_TE_A_217:1.778998
[2 : mcmc] Initial point: logA:3.053152, ns:0.9673207, theta_MC_100:1.041031, ombh2:0.02261756, omch2:0.1181184, w:-0.5284601, wa:-1.067179, beta_DE:-0.1792859, tau:0.05428552, A_planck:1.00607, calib_100T:0.999004, calib_217T:0.9973739, A_cib_217:43.30606, xi_sz_cib:0.8543824, A_sz:7.380057, ksz_norm:5.465039, gal545_A_100:8.055132, gal545_A_143:8.634407, gal545_A_143_217:20.04511, gal545_A_217:95.96707, ps_A_100_100:237.5486, ps_A_143_143:50.97938, ps_A_143_217:49.18257, ps_A_217_217:118.6227, galf_TE_A_100:0.07927665, galf_TE_A_100_143:0.154271, galf_TE_A_100_217:0.4925294, galf_TE_A_143:0.2623883, galf_TE_A_143_217:0.5033705, galf_TE_A_217:1.517854
[3 : mcmc] Initial point: logA:3.056022, ns:0.9692743, theta_MC_100:1.040824, ombh2:0.02241351, omch2:0.1188571, w:-1.266095, wa:0.7256591, beta_DE:3.282671, tau:0.0621888, A_planck:1.000196, calib_100T:0.9997243, calib_217T:0.9984451, A_cib_217:52.43785, xi_sz_cib:0.1302411, A_sz:4.00072, ksz_norm:1.466469, gal545_A_100:10.54589, gal545_A_143:9.133196, gal545_A_143_217:13.0226, gal545_A_217:86.17981, ps_A_100_100:265.5552, ps_A_143_143:44.25843, ps_A_143_217:37.2587, ps_A_217_217:112.1899, galf_TE_A_100:0.07381889, galf_TE_A_100_143:0.1261825, galf_TE_A_100_217:0.5318595, galf_TE_A_143:0.2150251, galf_TE_A_143_217:0.5907199, galf_TE_A_217:2.161133
[1 : mcmc] Initial point: logA:3.04775, ns:0.962773, theta_MC_100:1.041228, ombh2:0.02245598, omch2:0.1212719, w:-0.6427573, wa:-2.719415, beta_DE:3.532253, tau:0.05790187, A_planck:0.9965941, calib_100T:0.9997901, calib_217T:0.9989002, A_cib_217:33.40223, xi_sz_cib:0.8027809, A_sz:8.3444, ksz_norm:0.777214, gal545_A_100:7.980037, gal545_A_143:8.675902, gal545_A_143_217:23.6464, gal545_A_217:109.9813, ps_A_100_100:202.4068, ps_A_143_143:54.65369, ps_A_143_217:60.57461, ps_A_217_217:134.5354, galf_TE_A_100:0.1169624, galf_TE_A_100_143:0.09914549, galf_TE_A_100_217:0.2818448, galf_TE_A_143:0.200205, galf_TE_A_143_217:0.7595548, galf_TE_A_217:2.41347
[0 : mcmc] *WARNING* Parameter blocking manually/previously fixed: speeds will not be measured.
[0 : mcmc] Dragging with number of interpolating steps:
[0 : mcmc] *  1 : (['theta_MC_100', 'ombh2', 'omch2', 'w', 'wa', 'beta_DE', 'tau'], ['logA', 'ns'])
[0 : mcmc] * 12 : (['A_planck'], ['calib_100T', 'calib_217T', 'A_cib_217', 'xi_sz_cib', 'A_sz', 'ksz_norm', 'gal545_A_100', 'gal545_A_143', 'gal545_A_143_217', 'gal545_A_217', 'ps_A_100_100', 'ps_A_143_143', 'ps_A_143_217', 'ps_A_217_217', 'galf_TE_A_100', 'galf_TE_A_100_143', 'galf_TE_A_100_217', 'galf_TE_A_143', 'galf_TE_A_143_217', 'galf_TE_A_217'])
[0 : mcmc] Covariance matrix from previous sample.
[0 : mcmc] Sampling!
[1 : mcmc] Progress @ 2025-02-21 17:00:45 : 1 steps taken, and 1657 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:00:45 : 1 steps taken, and 1620 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:01:13 : 1 steps taken, and 1615 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:01:14 : 1 steps taken, and 1668 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:01:47 : 9 steps taken, and 1622 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:01:54 : 6 steps taken, and 1657 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:02:15 : 8 steps taken, and 1668 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:02:25 : 13 steps taken, and 1616 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:02:48 : 17 steps taken, and 1623 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:02:56 : 18 steps taken, and 1657 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:03:26 : 23 steps taken, and 1618 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:03:27 : 16 steps taken, and 1671 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:03:57 : 25 steps taken, and 1659 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:04:00 : 26 steps taken, and 1625 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:04:27 : 30 steps taken, and 1619 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:04:28 : 22 steps taken, and 1671 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:05:01 : 34 steps taken, and 1627 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:05:04 : 31 steps taken, and 1659 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:05:32 : 38 steps taken, and 1621 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:05:39 : 29 steps taken, and 1671 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:06:03 : 41 steps taken, and 1628 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:06:09 : 39 steps taken, and 1663 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:06:36 : 48 steps taken, and 1624 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:06:39 : 36 steps taken, and 1671 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:07:04 : 47 steps taken, and 1628 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:07:11 : 48 steps taken, and 1666 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:07:40 : 42 steps taken, and 1673 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:07:41 : 54 steps taken, and 1625 accepted.
[node147:64394:0:64394] Caught signal 11 (Segmentation fault: address not mapped to object at address 0xfffffffc05d37018)
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
BFD: DWARF error: can't find .debug_ranges section.
==== backtrace (tid:  64394) ====
 0 0x0000000000021563 ucs_debug_print_backtrace()  /dev/shm/UCX/1.8.0/GCCcore-9.3.0/ucx-1.8.0/src/ucs/debug/debug.c:653
 1 0x0000000000039f63 __massivenu_MOD_thermalnubackground_rho()  ???:0
 2 0x000000000003da49 __results_MOD_grho_no_de()  ???:0
 3 0x00000000000a5f01 dtauda_()  ???:0
 4 0x000000000004cfdc __results_MOD_thermo_init._omp_fn.0()  results.f90:0
 5 0x0000000000014295 GOMP_parallel_sections()  ???:0
 6 0x000000000004f7c2 __results_MOD_thermo_init()  ???:0
 7 0x00000000000f3923 __cambmain_MOD_initvars()  ???:0
 8 0x00000000000f9bff __cambmain_MOD_cmbmain()  ???:0
 9 0x0000000000105846 __camb_MOD_camb_getresults()  ???:0
10 0x00000000001104df __handles_MOD_cambdata_gettransfers()  ???:0
11 0x0000000000006a4a ffi_call_unix64()  :0
12 0x0000000000005fea ffi_call_int()  ffi64.c:0
13 0x00000000000091e0 _ctypes_callproc.cold()  :0
14 0x00000000000126ee PyCFuncPtr_call()  :0
15 0x00000000004f061c _PyObject_MakeTpCall()  ???:0
16 0x00000000004ec4f7 _PyEval_EvalFrameDefault()  ???:0
17 0x00000000004e694a _PyEval_EvalCode()  :0
18 0x000000000050500d method_vectorcall()  :0
19 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
20 0x00000000004e694a _PyEval_EvalCode()  :0
21 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
22 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
23 0x00000000004e694a _PyEval_EvalCode()  :0
24 0x000000000050500d method_vectorcall()  :0
25 0x0000000000505744 PyObject_Call()  ???:0
26 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
27 0x00000000004e694a _PyEval_EvalCode()  :0
28 0x000000000050500d method_vectorcall()  :0
29 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
30 0x00000000004e694a _PyEval_EvalCode()  :0
31 0x000000000050500d method_vectorcall()  :0
32 0x00000000004e8a8b _PyEval_EvalFrameDefault()  ???:0
33 0x00000000004e694a _PyEval_EvalCode()  :0
34 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
35 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
36 0x00000000004f8073 function_code_fastcall()  :0
37 0x0000000000504f01 method_vectorcall()  :0
38 0x00000000004ec5b4 _PyEval_EvalFrameDefault()  ???:0
39 0x00000000004f8073 function_code_fastcall()  :0
40 0x00000000004e7edc _PyEval_EvalFrameDefault()  ???:0
41 0x00000000004e694a _PyEval_EvalCode()  :0
42 0x00000000004f7d95 _PyFunction_Vectorcall()  ???:0
43 0x0000000000505744 PyObject_Call()  ???:0
44 0x00000000004eac68 _PyEval_EvalFrameDefault()  ???:0
45 0x00000000004f8073 function_code_fastcall()  :0
46 0x00000000004e7c22 _PyEval_EvalFrameDefault()  ???:0
47 0x00000000004e694a _PyEval_EvalCode()  :0
48 0x00000000004e65d7 _PyEval_EvalCodeWithName()  ???:0
49 0x00000000004e6589 PyEval_EvalCodeEx()  ???:0
50 0x0000000000593f2b PyEval_EvalCode()  ???:0
51 0x00000000005c1697 run_eval_code_obj()  :0
52 0x00000000005bd6b0 run_mod()  :0
53 0x00000000004565eb pyrun_file.cold()  :0
54 0x00000000005b7392 PyRun_SimpleFileExFlags()  ???:0
55 0x00000000005b490e Py_RunMain()  ???:0
56 0x0000000000587fd9 Py_BytesMain()  ???:0
57 0x0000000000022555 __libc_start_main()  ???:0
=================================
[node147:64394] *** Process received signal ***
[node147:64394] Signal: Segmentation fault (11)
[node147:64394] Signal code:  (-6)
[node147:64394] Failing at address: 0x8cfb80000fb8a
[node147:64394] [ 0] /lib64/libpthread.so.0(+0xf630)[0x7f5557e64630]
[node147:64394] [ 1] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__massivenu_MOD_thermalnubackground_rho+0x63)[0x7f553675ef63]
[node147:64394] [ 2] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_grho_no_de+0xc9)[0x7f5536762a49]
[node147:64394] [ 3] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(dtauda_+0x61)[0x7f55367caf01]
[node147:64394] [ 4] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(+0x4cfdc)[0x7f5536771fdc]
[node147:64394] [ 5] /users/smp24dhl/.conda/envs/cosmos/bin/../lib/libgomp.so.1(GOMP_parallel_sections+0x80)[0x7f55366fa295]
[node147:64394] [ 6] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__results_MOD_thermo_init+0x12d2)[0x7f55367747c2]
[node147:64394] [ 7] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_initvars+0x193)[0x7f5536818923]
[node147:64394] [ 8] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__cambmain_MOD_cmbmain+0xbf)[0x7f553681ebff]
[node147:64394] [ 9] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__camb_MOD_camb_getresults+0x2986)[0x7f553682a846]
[node147:64394] [10] /users/smp24dhl/cosmo/code/CAMB_beta/camb/camblib.so(__handles_MOD_cambdata_gettransfers+0x1f)[0x7f55368354df]
[node147:64394] [11] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x6a4a)[0x7f555811fa4a]
[node147:64394] [12] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/../../libffi.so.8(+0x5fea)[0x7f555811efea]
[node147:64394] [13] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x91e0)[0x7f554de1e1e0]
[node147:64394] [14] /users/smp24dhl/.conda/envs/cosmos/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x126ee)[0x7f554de276ee]
[node147:64394] [15] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyObject_MakeTpCall+0x2ec)[0x4f061c]
[node147:64394] [16] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x4ca7)[0x4ec4f7]
[node147:64394] [17] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:64394] [18] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:64394] [19] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:64394] [20] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:64394] [21] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyFunction_Vectorcall+0xd5)[0x4f7d95]
[node147:64394] [22] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:64394] [23] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:64394] [24] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:64394] [25] /users/smp24dhl/.conda/envs/cosmos/bin/python(PyObject_Call+0xb4)[0x505744]
[node147:64394] [26] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x3418)[0x4eac68]
[node147:64394] [27] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x4e694a]
[node147:64394] [28] /users/smp24dhl/.conda/envs/cosmos/bin/python[0x50500d]
[node147:64394] [29] /users/smp24dhl/.conda/envs/cosmos/bin/python(_PyEval_EvalFrameDefault+0x123b)[0x4e8a8b]
[node147:64394] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 64394 on node node147 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
./chains/PlanckLensingDESI.1.txt
./chains/PlanckLensingDESI.4.txt
./chains/PlanckLensingDESI.2.txt
./chains/PlanckLensingDESI.3.txt
Removed 0.3 as burn in
R-1 = 1.0315719001085208
H_0 = 66.3^{+2.8}_{-3.2}
w_{0,\mathrm{DE}} = -0.59^{+0.30}_{-0.25}
w_{a,\mathrm{DE}} < -1.45
\beta_{\mathrm{DE}} = 2.5^{+1.1}_{-2.1}
triangle plot saved at ./chains/outputs/PlanckLensingDESI_triangle.png
parameters saved at ./chains/outputs/PlanckLensingDESI_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'PlanckLensingDESI'
[0 : output] Found existing info files with the requested output prefix: 'chains/PlanckLensingDESI'
[0 : output] Let's try to resume/load.
[0 : output] Found an old sample. Resuming.
[0 : prior] *WARNING* External prior 'SZ' loaded. Mind that it might not be normalized!
[0 : camb] `camb` module loaded successfully from /users/smp24dhl/cosmo/code/CAMB_beta/camb
[0 : planck_2018_highl_plik.ttteee] `clik` module loaded successfully from /users/smp24dhl/cosmo/code/planck/clik/lib/python3.9/site-packages/clik
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
----
clik version clik_16.0b1-2-g63c889f3aa9c MAKEFILE
  smica
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
----
Checking likelihood '/users/smp24dhl/cosmo/data/planck_2018/baseline/plc_3.0/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik' on test data. got -1172.47 expected -1172.47 (diff -4.34054e-07)
----
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
Loading ACT DR6 lensing likelihood v1.2...
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[3 : bao.desi_2024_bao_all] Initialized.
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[0 : bao.desi_2024_bao_all] Initialized.
[0 : mcmc] Resuming from previous sample!
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
/users/smp24dhl/cosmo/code/act_dr6_lenslike/act_dr6_lenslike/act_dr6_lenslike.py:416: UserWarning: Hartlap correction to cinv: 0.949874686716792
  warnings.warn(f"Hartlap correction to cinv: {hartlap_correction}")
[2 : bao.desi_2024_bao_all] Initialized.
[1 : bao.desi_2024_bao_all] Initialized.
[0 : samplecollection] Loaded 1625 sample points from 'chains/PlanckLensingDESI.1.txt'
[1 : samplecollection] Loaded 1666 sample points from 'chains/PlanckLensingDESI.2.txt'
[2 : samplecollection] Loaded 1628 sample points from 'chains/PlanckLensingDESI.3.txt'
[3 : samplecollection] Loaded 1673 sample points from 'chains/PlanckLensingDESI.4.txt'
[0 : mcmc] Initial point: logA:3.037251, ns:0.9620901, theta_MC_100:1.041131, ombh2:0.02230372, omch2:0.1207489, w:-0.09952694, wa:-2.061432, beta_DE:-0.3900007, tau:0.04890355, A_planck:1.001746, calib_100T:1.000205, calib_217T:0.9987564, A_cib_217:42.7653, xi_sz_cib:0.8980037, A_sz:4.5124, ksz_norm:5.925353, gal545_A_100:8.838483, gal545_A_143:15.7405, gal545_A_143_217:24.08421, gal545_A_217:107.0026, ps_A_100_100:289.0152, ps_A_143_143:43.06936, ps_A_143_217:44.17614, ps_A_217_217:109.5554, galf_TE_A_100:0.1282809, galf_TE_A_100_143:0.1329137, galf_TE_A_100_217:0.4121472, galf_TE_A_143:0.1308727, galf_TE_A_143_217:0.6512756, galf_TE_A_217:2.075208
[2 : mcmc] Initial point: logA:3.043343, ns:0.9674623, theta_MC_100:1.041113, ombh2:0.02268023, omch2:0.1188448, w:-0.5216761, wa:-1.04409, beta_DE:-0.3086239, tau:0.05162082, A_planck:1.003908, calib_100T:0.9990762, calib_217T:0.9977959, A_cib_217:57.54218, xi_sz_cib:0.1592698, A_sz:5.9968, ksz_norm:1.938493, gal545_A_100:9.047764, gal545_A_143:12.76051, gal545_A_143_217:16.55188, gal545_A_217:84.81454, ps_A_100_100:266.8589, ps_A_143_143:40.85821, ps_A_143_217:32.28728, ps_A_217_217:98.74913, galf_TE_A_100:0.1871643, galf_TE_A_100_143:0.1685857, galf_TE_A_100_217:0.3345439, galf_TE_A_143:0.180099, galf_TE_A_143_217:0.6968896, galf_TE_A_217:1.903618
[3 : mcmc] Initial point: logA:3.038121, ns:0.9684871, theta_MC_100:1.040839, ombh2:0.02239047, omch2:0.1189838, w:-1.107152, wa:-0.01680403, beta_DE:3.63053, tau:0.05249035, A_planck:1.000433, calib_100T:1.000217, calib_217T:0.9974549, A_cib_217:47.06377, xi_sz_cib:0.5186723, A_sz:4.798813, ksz_norm:2.110514, gal545_A_100:11.3399, gal545_A_143:10.7965, gal545_A_143_217:15.35965, gal545_A_217:89.61597, ps_A_100_100:281.6773, ps_A_143_143:42.59695, ps_A_143_217:45.82626, ps_A_217_217:117.6676, galf_TE_A_100:0.1414965, galf_TE_A_100_143:0.07683682, galf_TE_A_100_217:0.5593091, galf_TE_A_143:0.1694343, galf_TE_A_143_217:0.5433871, galf_TE_A_217:2.30999
[1 : mcmc] Initial point: logA:3.031925, ns:0.9621195, theta_MC_100:1.04154, ombh2:0.02232747, omch2:0.1205141, w:-0.5268903, wa:-2.820514, beta_DE:4.29612, tau:0.05315319, A_planck:0.995697, calib_100T:1.000125, calib_217T:0.9995192, A_cib_217:52.35666, xi_sz_cib:0.1926261, A_sz:2.407916, ksz_norm:8.694208, gal545_A_100:9.691894, gal545_A_143:12.09618, gal545_A_143_217:16.45394, gal545_A_217:91.18447, ps_A_100_100:311.1148, ps_A_143_143:42.90822, ps_A_143_217:26.19003, ps_A_217_217:98.0687, galf_TE_A_100:0.1821, galf_TE_A_100_143:0.1427637, galf_TE_A_100_217:0.4481545, galf_TE_A_143:0.245344, galf_TE_A_143_217:0.7405695, galf_TE_A_217:2.244762
[0 : mcmc] *WARNING* Parameter blocking manually/previously fixed: speeds will not be measured.
[0 : mcmc] Dragging with number of interpolating steps:
[0 : mcmc] *  1 : (['theta_MC_100', 'ombh2', 'omch2', 'w', 'wa', 'beta_DE', 'tau'], ['logA', 'ns'])
[0 : mcmc] * 12 : (['A_planck'], ['calib_100T', 'calib_217T', 'A_cib_217', 'xi_sz_cib', 'A_sz', 'ksz_norm', 'gal545_A_100', 'gal545_A_143', 'gal545_A_143_217', 'gal545_A_217', 'ps_A_100_100', 'ps_A_143_143', 'ps_A_143_217', 'ps_A_217_217', 'galf_TE_A_100', 'galf_TE_A_100_143', 'galf_TE_A_100_217', 'galf_TE_A_143', 'galf_TE_A_143_217', 'galf_TE_A_217'])
[0 : mcmc] Covariance matrix from previous sample.
[0 : mcmc] Sampling!
[1 : mcmc] Progress @ 2025-02-21 17:08:15 : 1 steps taken, and 1666 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:08:33 : 1 steps taken, and 1625 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:08:44 : 1 steps taken, and 1673 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:08:44 : 1 steps taken, and 1628 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:09:24 : 5 steps taken, and 1666 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:09:45 : 7 steps taken, and 1626 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:09:45 : 9 steps taken, and 1673 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:09:49 : 7 steps taken, and 1628 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:10:27 : 13 steps taken, and 1668 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:10:52 : 13 steps taken, and 1628 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:10:54 : 16 steps taken, and 1630 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:10:56 : 15 steps taken, and 1673 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:11:38 : 19 steps taken, and 1672 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:11:57 : 22 steps taken, and 1676 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:12:02 : 23 steps taken, and 1630 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:12:06 : 24 steps taken, and 1631 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:12:40 : 26 steps taken, and 1674 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:13:07 : 29 steps taken, and 1634 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:13:09 : 28 steps taken, and 1676 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:13:11 : 32 steps taken, and 1634 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:13:51 : 32 steps taken, and 1677 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:14:08 : 36 steps taken, and 1635 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:14:13 : 40 steps taken, and 1634 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:14:17 : 34 steps taken, and 1676 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:14:52 : 38 steps taken, and 1679 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:15:17 : 41 steps taken, and 1635 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:15:19 : 41 steps taken, and 1676 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:15:24 : 46 steps taken, and 1634 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:15:54 : 48 steps taken, and 1680 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:16:26 : 52 steps taken, and 1637 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:16:30 : 49 steps taken, and 1638 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:16:30 : 47 steps taken, and 1677 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:16:55 : 56 steps taken, and 1680 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:17:27 : 58 steps taken, and 1639 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:17:32 : 53 steps taken, and 1678 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:17:39 : 60 steps taken, and 1641 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:17:56 : 67 steps taken, and 1682 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:18:27 : 62 steps taken, and 1639 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:18:32 : 60 steps taken, and 1678 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:18:47 : 68 steps taken, and 1641 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:19:07 : 75 steps taken, and 1682 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:19:36 : 67 steps taken, and 1640 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:19:40 : 66 steps taken, and 1678 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:19:48 : 75 steps taken, and 1642 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:20:12 : 85 steps taken, and 1686 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:20:38 : 75 steps taken, and 1641 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:20:40 : 73 steps taken, and 1680 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:20:58 : 85 steps taken, and 1644 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:21:22 : 92 steps taken, and 1686 accepted.
[3 : mcmc] Progress @ 2025-02-21 17:21:41 : 80 steps taken, and 1681 accepted.
[2 : mcmc] Progress @ 2025-02-21 17:21:49 : 87 steps taken, and 1642 accepted.
[0 : mcmc] Progress @ 2025-02-21 17:21:58 : 91 steps taken, and 1644 accepted.
[1 : mcmc] Progress @ 2025-02-21 17:22:26 : 102 steps taken, and 1690 accepted.
