./chains/large.3.txt
./chains/large.4.txt
./chains/large.1.txt
./chains/large.2.txt
Removed 0.3 as burn in
R-1 = 4.817447428873822
H_0 = 68.05\pm 0.72
w_{0,\mathrm{DE}} = -0.830\pm 0.063
w_{a,\mathrm{DE}} = -0.72^{+0.30}_{-0.25}
\beta_{\mathrm{DE}} = 0.94^{+0.35}_{-0.30}
triangle plot saved at ./chains/outputs/large_triangle.png
parameters saved at ./chains/outputs/large_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'large'
[0 : output] Found existing info files with the requested output prefix: 'chains/large'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/large.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
R-1 = 4.817447428873822
H_0 = 68.05\pm 0.72
w_{0,\mathrm{DE}} = -0.830\pm 0.063
w_{a,\mathrm{DE}} = -0.72^{+0.30}_{-0.25}
\beta_{\mathrm{DE}} = 0.94^{+0.35}_{-0.30}
triangle plot saved at ./chains/outputs/large_triangle.png
parameters saved at ./chains/outputs/large_parameters.txt
chain not converged
[0 : output] Output to be read-from/written-into folder 'chains', with prefix 'large'
[0 : output] Found existing info files with the requested output prefix: 'chains/large'
[0 : output] Let's try to resume/load.
[0 : output] *ERROR* File chains/large.input.yaml.locked is locked.
You may be running multiple jobs with the same output when you intended to run with MPI. Check that mpi4py is correctly installed and configured (using the same mpi as mpirun/mpiexec); e.g. try the test at
https://cobaya.readthedocs.io/en/latest/installation.html#mpi-parallelization-optional-but-encouraged
Your current mpi4py config is:
 {}
If this is a lock issue you can disable this check by setting env COBAYA_USE_FILE_LOCKING=False.
[0 : run] Aborting MPI due to error
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
./chains/large.3.txt
./chains/large.4.txt
./chains/large.1.txt
./chains/large.2.txt
Removed 0.3 as burn in
slurmstepd: error: *** JOB 5462713 ON node075 CANCELLED AT 2025-02-20T17:16:34 ***
